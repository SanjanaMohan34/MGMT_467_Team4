{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SukrH0tPeFZ2",
        "178da864",
        "859fc125",
        "ZMuHLop8Q0G6"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjanaMohan34/MGMT_467_Team4/blob/main/Final_Project_Bitcoin_Price_Prediction_Pipeline/Individual_DIVE_Journals/Final_Lily_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DIVE Analysis**\n",
        "\n",
        "**Discover:** What is the current Bitcoin valuation? Connect to the CoinCap API to fetch current price and asset information for Bitcoin. (Cells 1-6)\n",
        "\n",
        "*Challenge*: The version of CoinCap's API changed from when we created the project blueprint. This meant the domain I was used failed to connect to the server's IP address. This was resolved by investigating CoinCap's webpage and updating the API url. However, this change immediately exposed a new challenge: the v3 API required authentication, leading to 401: Unauthorized and 403: Forbidden errors. I addressed this by sourcing and integrating a CoinCap API key into the method.\n",
        "\n",
        "Current server: https://rest.coincap.io/v3/assets\n",
        "\n",
        "Match the schema with the following from https://pro.coincap.io/api-docs/ : { \"timestamp\": 0, \"data\": [ { \"id\": \"string\", \"rank\": \"string\", \"symbol\": \"string\", \"name\": \"string\", \"supply\": \"string\", \"maxSupply\": \"string\", \"marketCapUsd\": \"string\", \"volumeUsd24Hr\": \"string\", \"priceUsd\": \"string\", \"changePercent24Hr\": \"string\", \"vwap24Hr\": \"string\", \"explorer\": \"string\", \"tokens\": { \"additionalProp1\": [ \"string\" ], \"additionalProp2\": [ \"string\" ], \"additionalProp3\": [ \"string\" ] } } ] }\n",
        "\n",
        "**Investigate:** What are historical Bitcoin prices? Use the endpoint /assets/{slug}/history to fetch historical data from CoinCap's API for Bitcoin. Retrieve data for the maximum amount of time allowed by the API - 1 year. (Historical Data)\n",
        "\n",
        "**Visualize:** What trends are observable in Bitcoin prices over 2025? Command: \"Generate minimal python and use plotly and the variables defined in this notebook to generate one interactive chart displaying the Bitcoin price over time for the year of data that was fetched into the GCP table directed-bongo-471119-d1.467_final_3_2.data_1_t.\" (Visualization)\n",
        "\n",
        "\n",
        "**Extend:** Build a linear regression model to predict Bitcoin price. What other types of models could be used to predict price with historical price data? [ARIMA (AutoRegressive Integrated Moving Average), Exponential Smoothing, Recurrent Nueral Networks, Tree-based Models (Random Forest, XGBoost)]"
      ],
      "metadata": {
        "id": "8-9YTo1ggubS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 1: Python packages and authentication"
      ],
      "metadata": {
        "id": "SukrH0tPeFZ2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8908b17f",
        "outputId": "a46ba015-0fe6-4c89-8a56-e769b16fa205"
      },
      "source": [
        "R\"\"\"This cell installs required Python packages and authenticates the user to Google Cloud.\"\"\"\n",
        "!pip install google-cloud-storage google-cloud-bigquery requests\n",
        "\n",
        "from google.colab import auth\n",
        "print(\"Authenticating to Google Cloud...\")\n",
        "auth.authenticate_user()\n",
        "print(\"Authentication successful.\")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.12/dist-packages (3.7.0)\n",
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.12/dist-packages (3.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.26.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.43.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.28.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.5.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.8.0)\n",
            "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (1.7.1)\n",
            "Requirement already satisfied: packaging>=24.2.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (25.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage) (1.72.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage) (1.26.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.71.2)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (4.9.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (4.15.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.26.1->google-cloud-storage) (0.6.1)\n",
            "Authenticating to Google Cloud...\n",
            "Authentication successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "178da864"
      },
      "source": [
        "## Cell 2: Configure  project-specific variables and set the `gcloud` project.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7ab0f07",
        "outputId": "55074d0b-11de-4579-a8a8-0c3bff8bc9ce"
      },
      "source": [
        "R\"\"\"\n",
        "This cell configures essential project-specific variables for Google Cloud operations.\n",
        "\n",
        "It defines:\n",
        "- `PROJECT_ID`: The Google Cloud project ID.\n",
        "- `GCP_REGION`: The Google Cloud region for services.\n",
        "- `GCS_BUCKET_NAME`: The name of the Google Cloud Storage bucket.\n",
        "- `GCS_FOLDER_PATH`: The folder path within the GCS bucket for data storage.\n",
        "- `BQ_DATASET`: The BigQuery dataset name.\n",
        "- `BQ_TABLE`: The BigQuery table name for flight data.\n",
        "- `FLIGHT_RECORD_LIMIT`: A pipeline setting to limit records from the API.\n",
        "\n",
        "Finally, it sets the `gcloud` project configuration to the specified `PROJECT_ID`.\n",
        "R\"\"\"\n",
        "# --- !! CONFIGURE YOUR VARIABLES !! ---\n",
        "\n",
        "PROJECT_ID = \"directed-bongo-471119-d1\"\n",
        "GCP_REGION = \"us-central1\"  # Or the region you are using\n",
        "\n",
        "# --- GCS Bucket (Source & Target) ---\n",
        "GCS_BUCKET_NAME = \"467_final_3_2\"\n",
        "GCS_FOLDER_PATH = \"data_1\" # The folder you set in your scheduler\n",
        "\n",
        "# --- BigQuery Table (Target) ---\n",
        "BQ_DATASET = \"467_final_3_2\" # The dataset you created\n",
        "BQ_TABLE = \"data_1_t\"        # The table for flight data\n",
        "\n",
        "# --- Pipeline Settings ---\n",
        "FLIGHT_RECORD_LIMIT = 500 # How many records to pull from the API\n",
        "\n",
        "# --- CoinCap API Key ---\n",
        "COINCAP_API_KEY = \"7aaeaf52f203c5f61c86c294210dada652fc5adc8d566875faa9eedebbbdf761\" # Replace with your actual CoinCap API key\n",
        "\n",
        "# Define BigQuery Schema for Historical Data\n",
        "BQ_SCHEMA = [\n",
        "    bigquery.SchemaField(\"id\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"priceUsd\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"coincap_query_time_bq\", \"TIMESTAMP\", mode=\"NULLABLE\")\n",
        "]\n",
        "\n",
        "# -------------------------------------\n",
        "\n",
        "# Set the project for all gcloud commands\n",
        "!gcloud config set project $PROJECT_ID\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "859fc125"
      },
      "source": [
        "## Cell 3: Define the `OpenSkyApi` class and helper functions for data parsing and formatting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe4d60cb",
        "outputId": "fb1df3b2-7210-4c8c-d598-0946a2c0cb89"
      },
      "source": [
        "R\"\"\"This cell defines the `CoinCapApi` class and helper functions for interacting with the CoinCap API.\"\"\"\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import datetime\n",
        "import time\n",
        "import requests\n",
        "from collections import defaultdict\n",
        "from google.cloud import storage, bigquery\n",
        "\n",
        "# ==============================================================================\n",
        "# CoinCap API Library Code\n",
        "# ==============================================================================\n",
        "\n",
        "logger = logging.getLogger('coincap_api')\n",
        "logger.addHandler(logging.NullHandler())\n",
        "\n",
        "class CoinCapAsset(object):\n",
        "    \"\"\"Represents a single cryptocurrency asset from the CoinCap API data.\"\"\"\n",
        "    keys = [\"id\", \"rank\", \"symbol\", \"name\", \"supply\", \"maxSupply\",\n",
        "            \"marketCapUsd\", \"volumeUsd24Hr\", \"priceUsd\", \"changePercent24Hr\",\n",
        "            \"vwap24Hr\", \"explorer\"]\n",
        "\n",
        "    def __init__(self, data_dict):\n",
        "        for key in CoinCapAsset.keys:\n",
        "            setattr(self, key, data_dict.get(key))\n",
        "\n",
        "class CoinCapResponse(object):\n",
        "    \"\"\"Represents the overall response from the CoinCap API for an asset.\"\"\"\n",
        "    def __init__(self, j):\n",
        "        self.__dict__ = j\n",
        "        if 'data' in self.__dict__ and self.data is not None:\n",
        "            if isinstance(self.data, list) and len(self.data) > 0:\n",
        "                self.data = CoinCapAsset(self.data[0])\n",
        "            elif isinstance(self.data, dict):\n",
        "                self.data = CoinCapAsset(self.data)\n",
        "            else:\n",
        "                self.data = None\n",
        "        else:\n",
        "            self.data = None\n",
        "        if 'timestamp' in self.__dict__ and self.timestamp is not None:\n",
        "            self.timestamp_seconds = int(self.timestamp / 1000)\n",
        "        else:\n",
        "            self.timestamp_seconds = None\n",
        "\n",
        "\n",
        "class CoinCapApi(object):\n",
        "    def __init__(self):\n",
        "        self._api_url = \"https://rest.coincap.io/v3\"\n",
        "        self._last_requests = defaultdict(lambda: 0)\n",
        "        self._rate_limit_seconds = 60 / 2000 # 0.03 seconds per request\n",
        "\n",
        "    def _get_json(self, url_post, callee, params=None):\n",
        "        time_since_last_request = time.time() - self._last_requests[callee]\n",
        "        if time_since_last_request < self._rate_limit_seconds:\n",
        "            sleep_time = self._rate_limit_seconds - time_since_last_request\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'application/json',\n",
        "            'Authorization': f'Bearer {COINCAP_API_KEY}'\n",
        "        }\n",
        "\n",
        "        r = requests.get(f\"{self._api_url}{url_post}\", headers=headers, params=params, timeout=60.00)\n",
        "        if r.status_code == 200:\n",
        "            self._last_requests[callee] = time.time()\n",
        "            json_response = r.json()\n",
        "            return json_response\n",
        "        logger.debug(f\"Response not OK. Status {r.status_code} - {r.reason}\")\n",
        "        print(f\"DEBUG: API call to {f'{self._api_url}{url_post}'} failed with status {r.status_code}: {r.reason}. Response text: {r.text}\")\n",
        "        return None\n",
        "\n",
        "    def get_asset(self, asset_id=\"bitcoin\"):\n",
        "        \"\"\"Fetches data for a specific cryptocurrency asset.\"\"\"\n",
        "        json_data = self._get_json(f\"/assets/{asset_id}\", self.get_asset)\n",
        "        return CoinCapResponse(json_data) if json_data else None\n",
        "\n",
        "    def get_asset_history(self, asset_id, interval, start=None, end=None):\n",
        "        \"\"\"Fetches historical data for a specific cryptocurrency asset.\n",
        "\n",
        "        Args:\n",
        "            asset_id (str): The ID of the asset (e.g., \"bitcoin\").\n",
        "            interval (str): The time interval for historical data (e.g., 'd1', 'h1', 'm1').\n",
        "            start (int, optional): Unix timestamp (milliseconds) for the start of the period.\n",
        "            end (int, optional): Unix timestamp (milliseconds) for the end of the period.\n",
        "\n",
        "        Returns:\n",
        "            dict: The JSON response containing historical data, or None if an error occurs.\n",
        "        \"\"\"\n",
        "        url_post = f\"/assets/{asset_id}/history\"\n",
        "        params = {'interval': interval}\n",
        "\n",
        "        if start is not None:\n",
        "            params['start'] = int(start)\n",
        "        if end is not None:\n",
        "            params['end'] = int(end)\n",
        "\n",
        "        json_data = self._get_json(url_post, self.get_asset_history, params=params)\n",
        "        return json_data\n",
        "\n",
        "# ==============================================================================\n",
        "# Data Parser Functions\n",
        "# ==============================================================================\n",
        "\n",
        "def _convertTimestamp(timestamp_seconds):\n",
        "    \"\"\"Converts a Unix timestamp (in seconds) to a formatted datetime string.\"\"\"\n",
        "    if timestamp_seconds is not None:\n",
        "        try:\n",
        "            return datetime.datetime.fromtimestamp(timestamp_seconds).strftime('%Y-%m-%d %H:%M:%S')\n",
        "        except Exception:\n",
        "            logger.error(f\"Error converting timestamp {timestamp_seconds}\", exc_info=True)\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "def _convert(data, dataType):\n",
        "    \"\"\"Converts data to the specified dataType, handling None and type conversion errors.\"\"\"\n",
        "    if data is None:\n",
        "        return None\n",
        "    if dataType == str:\n",
        "        return str(data).strip()\n",
        "    try:\n",
        "        return dataType(data)\n",
        "    except Exception:\n",
        "        logger.error(f\"Error converting data '{data}' to {dataType.__name__}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "def _convertRow(coincap_response):\n",
        "    \"\"\"Converts a CoinCapResponse object into a dictionary suitable for BigQuery.\n",
        "    Assumes coincap_response contains a CoinCapAsset object under .data and a timestamp_seconds.\n",
        "    \"\"\"\n",
        "    if coincap_response is None or coincap_response.data is None:\n",
        "        return {}\n",
        "\n",
        "    asset_data = coincap_response.data\n",
        "    row = {\n",
        "        'id': _convert(asset_data.id, str),\n",
        "        'rank': _convert(asset_data.rank, int),\n",
        "        'symbol': _convert(asset_data.symbol, str),\n",
        "        'name': _convert(asset_data.name, str),\n",
        "        'supply': _convert(asset_data.supply, float),\n",
        "        'maxSupply': _convert(asset_data.maxSupply, float),\n",
        "        'marketCapUsd': _convert(asset_data.marketCapUsd, float),\n",
        "        'volumeUsd24Hr': _convert(asset_data.volumeUsd24Hr, float),\n",
        "        'priceUsd': _convert(asset_data.priceUsd, float),\n",
        "        'changePercent24Hr': _convert(asset_data.changePercent24Hr, float),\n",
        "        'vwap24Hr': _convert(asset_data.vwap24Hr, float),\n",
        "        'explorer': _convert(asset_data.explorer, str)\n",
        "    }\n",
        "\n",
        "    if coincap_response.timestamp_seconds is not None:\n",
        "        row['coincap_query_time_bq'] = _convertTimestamp(coincap_response.timestamp_seconds)\n",
        "\n",
        "    return {k: v for k, v in row.items() if v is not None}\n",
        "\n",
        "def _convertHistoricalRecord(historical_data_point, asset_id):\n",
        "    \"\"\"Converts a single historical data point into a dictionary suitable for BigQuery.\"\"\"\n",
        "    if not historical_data_point:\n",
        "        return {}\n",
        "\n",
        "    price_usd = _convert(historical_data_point.get('priceUsd'), float)\n",
        "    timestamp_ms = historical_data_point.get('time')\n",
        "    coincap_query_time_bq = None\n",
        "    if timestamp_ms is not None:\n",
        "        coincap_query_time_bq = _convertTimestamp(timestamp_ms / 1000)\n",
        "\n",
        "    record = {\n",
        "        'id': asset_id,\n",
        "        'priceUsd': price_usd,\n",
        "        'coincap_query_time_bq': coincap_query_time_bq\n",
        "    }\n",
        "\n",
        "    return {k: v for k, v in record.items() if v is not None}\n",
        "\n",
        "print(\"âœ… CoinCap API classes and helper functions consolidated.\")\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… CoinCap API classes and helper functions consolidated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ffba020"
      },
      "source": [
        "## Cell 4: Define the `OpenSkyApi` class and helper functions, initialize GCP clients, define the BigQuery schema, and implement the data pipeline logic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9a6095e"
      },
      "source": [
        "## Adjust API Base URL (Retry with API Key)\n",
        "\n",
        "### Subtask:\n",
        "Integrate the CoinCap API key into the `CoinCapApi` class to resolve the `401: Unauthorized` error and re-execute the pipeline to fetch Bitcoin asset data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a41b590e"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining the `COINCAP_API_KEY` variable before it can be used in the `CoinCapApi` class. I will define it with a placeholder and instruct the user to replace it with their actual key.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3f0cec3",
        "outputId": "4ffa2d87-eedf-46ae-be67-72ec882107bc"
      },
      "source": [
        "COINCAP_API_KEY = \"7aaeaf52f203c5f61c86c294210dada652fc5adc8d566875faa9eedebbbdf761\" # Replace with your actual CoinCap API key\n",
        "print(\"âœ… COINCAP_API_KEY variable defined.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… COINCAP_API_KEY variable defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "728729ab"
      },
      "source": [
        "**Reasoning**:\n",
        "I will redefine the `CoinCapApi` class to include the `Authorization` header using the `COINCAP_API_KEY`, and also redefine the necessary helper functions and the `run_full_pipeline_without_bq_load` function to ensure all components are updated and interdependent correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35f5097d"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `CoinCapApi` class has been redefined to include the API key, I will execute the `run_full_pipeline_without_bq_load()` function to verify if the API call is successful and data is fetched.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6ff6f7b"
      },
      "source": [
        "## Cell 6: Orchestrate the full data pipeline from API to GCS to BigQuery.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "426ba363",
        "outputId": "0809b05e-cd5b-4638-e8e4-2b653f6bb8c6"
      },
      "source": [
        "R\"\"\"This cell orchestrates the entire data pipeline, from API to GCS to BigQuery.\"\"\"\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import datetime\n",
        "import time\n",
        "import requests\n",
        "from collections import defaultdict\n",
        "from google.cloud import storage, bigquery\n",
        "from google.cloud.exceptions import NotFound # Import NotFound for specific error handling\n",
        "\n",
        "# Re-initialize GCP clients (ensuring they are available in this scope)\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "def load_gcs_to_bigquery(gcs_uri, project_id, bq_dataset, bq_table, bq_schema, bq_client_instance):\n",
        "    \"\"\"Loads data from a GCS URI into a BigQuery table.\"\"\"\n",
        "    print(f\"\\nStep: Loading data from GCS into BigQuery...\")\n",
        "    print(f\"  > Source: {gcs_uri}\")\n",
        "    print(f\"  > Target: {bq_dataset}.{bq_table}\")\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
        "    job_config.schema = bq_schema\n",
        "    job_config.autodetect = False\n",
        "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
        "\n",
        "    load_job = bq_client_instance.load_table_from_uri(\n",
        "        gcs_uri,\n",
        "        f\"{project_id}.{bq_dataset}.{bq_table}\",\n",
        "        job_config=job_config\n",
        "    )\n",
        "\n",
        "    print(f\"  > Starting BQ Load Job: {load_job.job_id}\")\n",
        "    load_job.result()\n",
        "    print(f\"  > Job complete. Loaded {load_job.output_rows} rows.\")\n",
        "    print(\"âœ… GCS to BigQuery Load Finished Successfully.\")\n",
        "\n",
        "def run_full_pipeline_without_bq_load():\n",
        "    \"\"\"Executes the API -> GCS pipeline for CoinCap historical data and returns GCS URI.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # ======================================================\n",
        "        # 1. Download data from CoinCap API\n",
        "        # ======================================================\n",
        "        print(f\"Step 1: Fetching Bitcoin historical data from CoinCap API...\")\n",
        "        api = CoinCapApi()\n",
        "        queryTime = datetime.datetime.now().timestamp()\n",
        "\n",
        "        asset_id_to_fetch = \"bitcoin\"\n",
        "\n",
        "        # Retrieve data for the maximum amount of time allowed by the API - 1 year (365 days)\n",
        "        end_datetime = datetime.datetime.now()\n",
        "        end_timestamp_ms = int(end_datetime.timestamp() * 1000)\n",
        "        start_datetime = end_datetime - datetime.timedelta(days=365)\n",
        "        start_timestamp_ms = int(start_datetime.timestamp() * 1000)\n",
        "\n",
        "        historical_data_response = api.get_asset_history(\n",
        "            asset_id=asset_id_to_fetch,\n",
        "            interval=\"d1\", # Daily interval\n",
        "            start=start_timestamp_ms,\n",
        "            end=end_timestamp_ms\n",
        "        )\n",
        "\n",
        "        records = []\n",
        "        if historical_data_response and 'data' in historical_data_response:\n",
        "            for data_point in historical_data_response['data']:\n",
        "                record = _convertHistoricalRecord(data_point, asset_id_to_fetch)\n",
        "                if record:\n",
        "                    records.append(record)\n",
        "\n",
        "        if not records:\n",
        "            print(\"No Bitcoin historical data found. Exiting.\")\n",
        "            return None\n",
        "\n",
        "        print(f\"  > Fetched {len(records)} record(s).\")\n",
        "\n",
        "        # ======================================================\n",
        "        # 2. Save data to GCS Bucket\n",
        "        # ======================================================\n",
        "        local_filename = \"bitcoin_historical_data.jsonl\"\n",
        "        with open(local_filename, 'w') as f:\n",
        "            for record in records:\n",
        "                f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "        gcs_filename = f\"{GCS_FOLDER_PATH}/coincap_history_batch_{int(queryTime)}.jsonl\"\n",
        "\n",
        "        print(f\"\\nStep 2: Uploading data to GCS...\")\n",
        "        print(f\"  > Source: {local_filename}\")\n",
        "        print(f\"  > Destination: gs://{GCS_BUCKET_NAME}/{gcs_filename}\")\n",
        "\n",
        "        bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
        "        blob = bucket.blob(gcs_filename)\n",
        "        blob.upload_from_filename(local_filename)\n",
        "\n",
        "        gcs_uri = f\"gs://{GCS_BUCKET_NAME}/{gcs_filename}\"\n",
        "        print(\"  > Upload complete.\")\n",
        "        print(\"âœ… API to GCS Pipeline Finished Successfully.\")\n",
        "        return gcs_uri\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ ERROR in pipeline: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"--- Running Full Data Pipeline (API -> GCS -> BigQuery) ---\")\n",
        "\n",
        "# 1. Ensure BigQuery table exists\n",
        "target_table_id = f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\"\n",
        "\n",
        "try:\n",
        "    bq_client.get_table(target_table_id)  # API request\n",
        "    print(f\"âœ… BigQuery table '{target_table_id}' already exists.\")\n",
        "except NotFound:\n",
        "    print(f\"BigQuery table '{target_table_id}' not found. Creating table...\")\n",
        "    table = bigquery.Table(target_table_id, schema=BQ_SCHEMA)\n",
        "    table = bq_client.create_table(table)  # API request\n",
        "    print(f\"âœ… Created table {table.project}.{table.dataset_id}.{table.table_id}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error checking or creating BigQuery table: {e}\")\n",
        "    exit() # Exit if table cannot be ensured\n",
        "\n",
        "# 2. Execute API -> GCS pipeline\n",
        "gcs_uri_for_bq_load = run_full_pipeline_without_bq_load()\n",
        "\n",
        "if gcs_uri_for_bq_load:\n",
        "    # 3. Load data from GCS to BigQuery\n",
        "    load_gcs_to_bigquery(\n",
        "        gcs_uri_for_bq_load,\n",
        "        PROJECT_ID,\n",
        "        BQ_DATASET,\n",
        "        BQ_TABLE,\n",
        "        BQ_SCHEMA,\n",
        "        bq_client\n",
        "    )\n",
        "    print(\"âœ… Pipeline Finished Successfully.\")\n",
        "else:\n",
        "    print(\"âŒ Pipeline aborted: No data fetched or GCS upload failed.\")\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Full Data Pipeline (API -> GCS -> BigQuery) ---\n",
            "âœ… BigQuery table 'directed-bongo-471119-d1.467_final_3_2.data_1_t' already exists.\n",
            "Step 1: Fetching Bitcoin historical data from CoinCap API...\n",
            "  > Fetched 366 record(s).\n",
            "\n",
            "Step 2: Uploading data to GCS...\n",
            "  > Source: bitcoin_historical_data.jsonl\n",
            "  > Destination: gs://467_final_3_2/data_1/coincap_history_batch_1765686807.jsonl\n",
            "  > Upload complete.\n",
            "âœ… API to GCS Pipeline Finished Successfully.\n",
            "\n",
            "Step: Loading data from GCS into BigQuery...\n",
            "  > Source: gs://467_final_3_2/data_1/coincap_history_batch_1765686807.jsonl\n",
            "  > Target: 467_final_3_2.data_1_t\n",
            "  > Starting BQ Load Job: 025658f5-4c44-4a0a-8ab4-22147264e1ed\n",
            "  > Job complete. Loaded 366 rows.\n",
            "âœ… GCS to BigQuery Load Finished Successfully.\n",
            "âœ… Pipeline Finished Successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "ZMuHLop8Q0G6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### Cell 9: Build, Evaluate, and Predict with a Linear Regression Model\n",
        "# @markdown **Objective:** Use BigQuery ML to create a linear regression model to predict the Bitcoin Close price.\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound # Import NotFound for specific error handling\n",
        "\n",
        "# Re-initialize the BigQuery client to ensure it's fresh and correctly scoped\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Re-define necessary variables as they might have been cleared\n",
        "BITCOIN_ANALYTICS_VIEW = \"bitcoin_analytics_view\"\n",
        "view_id = f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\"\n",
        "\n",
        "print(f\"Attempting to use view: {view_id}\")\n",
        "\n",
        "# --- Add a check for view existence before proceeding ---\n",
        "try:\n",
        "    client.get_table(view_id) # get_table works for views too\n",
        "    print(f\"âœ… BigQuery view '{view_id}' confirmed to exist.\")\n",
        "except NotFound:\n",
        "    print(f\"ðŸ”´ ERROR: BigQuery view '{view_id}' not found. Please ensure Cell 8 (Data Cleaning and Feature Engineering) ran successfully and created the view.\")\n",
        "    raise # Re-raise the error to stop execution if the view is truly not found\n",
        "except Exception as e:\n",
        "    print(f\"ðŸ”´ An unexpected error occurred while checking view existence: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- 1. Create the Linear Regression Model ---\n",
        "print(\"ðŸš€ Training Linear Regression model...\")\n",
        "create_linear_model_query = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{PROJECT_ID}.{BQ_DATASET}.bitcoin_price_predictor`\n",
        "OPTIONS(model_type='LINEAR_REG', input_label_cols=['priceUsd']) AS\n",
        "SELECT\n",
        "  priceUsd,\n",
        "  LAG(priceUsd, 1) OVER (ORDER BY coincap_query_time_bq) AS prev_priceUsd\n",
        "FROM\n",
        "  `{view_id}`\n",
        "WHERE\n",
        "  priceUsd IS NOT NULL\n",
        "  AND coincap_query_time_bq IS NOT NULL;\n",
        "\"\"\"\n",
        "linear_job = client.query(create_linear_model_query)\n",
        "linear_job.result()\n",
        "print(\"âœ… Linear Regression model created successfully.\")\n",
        "\n",
        "# --- 2. Evaluate the Model ---\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "evaluate_linear_model_query = f\"\"\"\n",
        "SELECT * FROM ML.EVALUATE(MODEL `{PROJECT_ID}.{BQ_DATASET}.bitcoin_price_predictor`);\n",
        "\"\"\"\n",
        "linear_eval_df = client.query(evaluate_linear_model_query).to_dataframe()\n",
        "display(linear_eval_df)\n",
        "\n",
        "# --- 3. Make Predictions with the Model ---\n",
        "print(\"\\n--- Sample Predictions ---\")\n",
        "predict_linear_query = f\"\"\"\n",
        "WITH PredictionInput AS (\n",
        "  SELECT\n",
        "    coincap_query_time_bq AS Timestamp,\n",
        "    priceUsd,\n",
        "    LAG(priceUsd, 1) OVER (ORDER BY coincap_query_time_bq) AS prev_priceUsd,\n",
        "    ROW_NUMBER() OVER (ORDER BY coincap_query_time_bq) AS rn\n",
        "  FROM\n",
        "    `{view_id}`\n",
        "  WHERE\n",
        "    priceUsd IS NOT NULL\n",
        "    AND coincap_query_time_bq IS NOT NULL\n",
        "),\n",
        "FilteredInput AS (\n",
        "  SELECT\n",
        "    Timestamp,\n",
        "    priceUsd,\n",
        "    prev_priceUsd,\n",
        "    rn\n",
        "  FROM\n",
        "    PredictionInput\n",
        "  WHERE\n",
        "    prev_priceUsd IS NOT NULL -- Exclude first row where prev_priceUsd is NULL\n",
        "  LIMIT 10\n",
        ")\n",
        "SELECT\n",
        "  fi.priceUsd AS actual_priceUsd,\n",
        "  p.predicted_priceUsd\n",
        "FROM\n",
        "  ML.PREDICT(MODEL `{PROJECT_ID}.{BQ_DATASET}.bitcoin_price_predictor`,\n",
        "    (SELECT rn, prev_priceUsd FROM FilteredInput)\n",
        "  ) AS p\n",
        "JOIN\n",
        "  FilteredInput AS fi\n",
        "ON\n",
        "  p.rn = fi.rn\n",
        "ORDER BY fi.Timestamp\n",
        "\"\"\"\n",
        "linear_predict_df = client.query(predict_linear_query).to_dataframe()\n",
        "display(linear_predict_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "xXbBG-C_BgfU",
        "outputId": "2373031f-93c2-4a15-836d-a0f88f77b87a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to use view: directed-bongo-471119-d1.467_final_3_2.data_1_t\n",
            "âœ… BigQuery view 'directed-bongo-471119-d1.467_final_3_2.data_1_t' confirmed to exist.\n",
            "ðŸš€ Training Linear Regression model...\n",
            "âœ… Linear Regression model created successfully.\n",
            "\n",
            "--- Model Evaluation ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   mean_absolute_error  mean_squared_error  mean_squared_log_error  \\\n",
              "0         1.159788e-08        1.345107e-16            1.590659e-26   \n",
              "\n",
              "   median_absolute_error  r2_score  explained_variance  \n",
              "0           1.159788e-08      -inf                 NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-506eb860-2d4d-4e25-ae7c-8d9df2f29a84\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_absolute_error</th>\n",
              "      <th>mean_squared_error</th>\n",
              "      <th>mean_squared_log_error</th>\n",
              "      <th>median_absolute_error</th>\n",
              "      <th>r2_score</th>\n",
              "      <th>explained_variance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.159788e-08</td>\n",
              "      <td>1.345107e-16</td>\n",
              "      <td>1.590659e-26</td>\n",
              "      <td>1.159788e-08</td>\n",
              "      <td>-inf</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-506eb860-2d4d-4e25-ae7c-8d9df2f29a84')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-506eb860-2d4d-4e25-ae7c-8d9df2f29a84 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-506eb860-2d4d-4e25-ae7c-8d9df2f29a84');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_c0c09bf5-2de4-4c57-baec-7635fa9cc82f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('linear_eval_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c0c09bf5-2de4-4c57-baec-7635fa9cc82f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('linear_eval_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "linear_eval_df",
              "repr_error": "Out of range float values are not JSON compliant: -inf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample Predictions ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [actual_priceUsd, predicted_priceUsd]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f6b00972-b50b-403d-a230-66c16dd51766\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual_priceUsd</th>\n",
              "      <th>predicted_priceUsd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f6b00972-b50b-403d-a230-66c16dd51766')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f6b00972-b50b-403d-a230-66c16dd51766 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f6b00972-b50b-403d-a230-66c16dd51766');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_cbb0beb0-1538-473c-bb6c-f142662080b1\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('linear_predict_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_cbb0beb0-1538-473c-bb6c-f142662080b1 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('linear_predict_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "linear_predict_df",
              "summary": "{\n  \"name\": \"linear_predict_df\",\n  \"rows\": 0,\n  \"fields\": [\n    {\n      \"column\": \"actual_priceUsd\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_priceUsd\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "512d2413"
      },
      "source": [
        "# Historical Data\n",
        "Implement `get_asset_history` in the `CoinCapApi` class to fetch historical asset data, handling `asset_id`, `interval`, `start`, and `end` parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9461a75a"
      },
      "source": [
        "## Implement get_asset_history in CoinCapApi\n",
        "\n",
        "### Subtask:\n",
        "Add a new method `get_asset_history` to the `CoinCapApi` class. This method will construct the request URL for `/assets/{asset_id}/history` and handle parameters like `interval`, `start`, and `end` to fetch historical data for a specified asset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32446d2d"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires adding a new method `get_asset_history` to the `CoinCapApi` class. I will modify the existing code block that defines the `CoinCapApi` class and related functions to include this new method as per the instructions, handling URL construction and parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2deb148",
        "outputId": "05c81552-602d-4daa-f16e-a5572c000628"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import datetime\n",
        "import time\n",
        "import requests\n",
        "from collections import defaultdict\n",
        "from google.cloud import storage, bigquery\n",
        "\n",
        "# --- !! CONFIGURE YOUR VARIABLES (re-defined for self-containment) !! ---\n",
        "PROJECT_ID = \"directed-bongo-471119-d1\"\n",
        "GCP_REGION = \"us-central1\"\n",
        "GCS_BUCKET_NAME = \"467_final_3_2\"\n",
        "GCS_FOLDER_PATH = \"data_1\"\n",
        "BQ_DATASET = \"467_final_3_2\"\n",
        "BQ_TABLE = \"data_1_t\"\n",
        "FLIGHT_RECORD_LIMIT = 500 # Not used in this specific CoinCap context, but kept for consistency if pipeline structure changes\n",
        "\n",
        "COINCAP_API_KEY = \"7aaeaf52f203c5f61c86c294210dada652fc5adc8d566875faa9eedebbbdf761\" # Re-define API key\n",
        "\n",
        "# Define BigQuery Schema based on _convertRow output\n",
        "BQ_SCHEMA = [\n",
        "    bigquery.SchemaField(\"id\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"rank\", \"INTEGER\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"symbol\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"name\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"supply\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"maxSupply\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"marketCapUsd\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"volumeUsd24Hr\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"priceUsd\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"changePercent24Hr\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"vwap24Hr\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"explorer\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"coincap_query_time_bq\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
        "]\n",
        "\n",
        "# ==============================================================================\n",
        "# CoinCap API Library Code (Modified to use API Key and add get_asset_history)\n",
        "# ==============================================================================\n",
        "\n",
        "logger = logging.getLogger('coincap_api')\n",
        "logger.addHandler(logging.NullHandler())\n",
        "\n",
        "class CoinCapAsset(object):\n",
        "    \"\"\"Represents a single cryptocurrency asset from the CoinCap API data.\"\"\"\n",
        "    keys = [\"id\", \"rank\", \"symbol\", \"name\", \"supply\", \"maxSupply\",\n",
        "            \"marketCapUsd\", \"volumeUsd24Hr\", \"priceUsd\", \"changePercent24Hr\",\n",
        "            \"vwap24Hr\", \"explorer\"]\n",
        "\n",
        "    def __init__(self, data_dict):\n",
        "        # Initialize attributes from the data_dict\n",
        "        for key in CoinCapAsset.keys:\n",
        "            # Use .get() to safely retrieve values, in case a key is missing\n",
        "            setattr(self, key, data_dict.get(key))\n",
        "\n",
        "class CoinCapResponse(object):\n",
        "    \"\"\"Represents the overall response from the CoinCap API for an asset.\"\"\"\n",
        "    def __init__(self, j):\n",
        "        self.__dict__ = j\n",
        "        # If 'data' exists and is not None, parse it into a CoinCapAsset object\n",
        "        if 'data' in self.__dict__ and self.data is not None:\n",
        "            self.data = CoinCapAsset(self.data)\n",
        "        else:\n",
        "            self.data = None\n",
        "        # Convert timestamp from milliseconds to seconds for general use\n",
        "        if 'timestamp' in self.__dict__ and self.timestamp is not None:\n",
        "            self.timestamp_seconds = int(self.timestamp / 1000)\n",
        "        else:\n",
        "            self.timestamp_seconds = None\n",
        "\n",
        "\n",
        "class CoinCapApi(object):\n",
        "    def __init__(self):\n",
        "        self._api_url = \"https://rest.coincap.io/v3\"\n",
        "        self._last_requests = defaultdict(lambda: 0)\n",
        "        # CoinCap has a rate limit of 2000 requests per minute, so ~0.03 seconds per request\n",
        "        self._rate_limit_seconds = 60 / 2000 # 0.03 seconds per request\n",
        "\n",
        "    def _get_json(self, url_post, callee, params=None):\n",
        "        # Implement a basic rate limit check\n",
        "        time_since_last_request = time.time() - self._last_requests[callee]\n",
        "        if time_since_last_request < self._rate_limit_seconds:\n",
        "            # Wait if we're hitting the rate limit too fast\n",
        "            sleep_time = self._rate_limit_seconds - time_since_last_request\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'application/json',\n",
        "            'Authorization': f'Bearer {COINCAP_API_KEY}'\n",
        "        }\n",
        "\n",
        "        r = requests.get(f\"{self._api_url}{url_post}\", headers=headers, params=params, timeout=60.00)\n",
        "        if r.status_code == 200:\n",
        "            self._last_requests[callee] = time.time()\n",
        "            json_response = r.json()\n",
        "            return json_response\n",
        "        logger.debug(f\"Response not OK. Status {r.status_code} - {r.reason}\")\n",
        "        print(f\"DEBUG: API call to {f'{self._api_url}{url_post}'} failed with status {r.status_code}: {r.reason}. Response text: {r.text}\")\n",
        "        return None\n",
        "\n",
        "    def get_asset(self, asset_id=\"bitcoin\"):\n",
        "        \"\"\"Fetches data for a specific cryptocurrency asset.\"\"\"\n",
        "        json_data = self._get_json(f\"/assets/{asset_id}\", self.get_asset)\n",
        "        return CoinCapResponse(json_data) if json_data else None\n",
        "\n",
        "    def get_asset_history(self, asset_id, interval, start=None, end=None):\n",
        "        \"\"\"Fetches historical data for a specific cryptocurrency asset.\n",
        "\n",
        "        Args:\n",
        "            asset_id (str): The ID of the asset (e.g., \"bitcoin\").\n",
        "            interval (str): The time interval for historical data (e.g., 'd1', 'h1', 'm1').\n",
        "            start (int, optional): Unix timestamp (milliseconds) for the start of the period.\n",
        "            end (int, optional): Unix timestamp (milliseconds) for the end of the period.\n",
        "\n",
        "        Returns:\n",
        "            dict: The JSON response containing historical data, or None if an error occurs.\n",
        "        \"\"\"\n",
        "        url_post = f\"/assets/{asset_id}/history\"\n",
        "        params = {'interval': interval}\n",
        "\n",
        "        if start is not None:\n",
        "            params['start'] = int(start)\n",
        "        if end is not None:\n",
        "            params['end'] = int(end)\n",
        "\n",
        "        json_data = self._get_json(url_post, self.get_asset_history, params=params)\n",
        "        return json_data\n",
        "\n",
        "# ==============================================================================\n",
        "# Data Parser Functions (re-defined to ensure consistency with updated CoinCapApi)\n",
        "# ==============================================================================\n",
        "\n",
        "def _convertTimestamp(timestamp_seconds):\n",
        "    \"\"\"Converts a Unix timestamp (in seconds) to a formatted datetime string.\"\"\"\n",
        "    if timestamp_seconds is not None:\n",
        "        try:\n",
        "            return datetime.datetime.fromtimestamp(timestamp_seconds).strftime('%Y-%m-%d %H:%M:%S')\n",
        "        except Exception:\n",
        "            logger.error(f\"Error converting timestamp {timestamp_seconds}\", exc_info=True)\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "def _convert(data, dataType):\n",
        "    \"\"\"Converts data to the specified dataType, handling None and type conversion errors.\"\"\"\n",
        "    if data is None:\n",
        "        return None\n",
        "    if dataType == str:\n",
        "        # For CoinCap API, some string values might already be clean, but strip() is safe.\n",
        "        return str(data).strip()\n",
        "    try:\n",
        "        return dataType(data)\n",
        "    except Exception:\n",
        "        logger.error(f\"Error converting data '{data}' to {dataType.__name__}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "def _convertRow(coincap_response):\n",
        "    \"\"\"Converts a CoinCapResponse object into a dictionary suitable for BigQuery.\n",
        "    Assumes coincap_response contains a CoinCapAsset object under .data and a timestamp_seconds.\n",
        "    \"\"\"\n",
        "    if coincap_response is None or coincap_response.data is None:\n",
        "        return {}\n",
        "\n",
        "    asset_data = coincap_response.data\n",
        "    row = {\n",
        "        'id': _convert(asset_data.id, str),\n",
        "        'rank': _convert(asset_data.rank, int),\n",
        "        'symbol': _convert(asset_data.symbol, str),\n",
        "        'name': _convert(asset_data.name, str),\n",
        "        'supply': _convert(asset_data.supply, float),\n",
        "        'maxSupply': _convert(asset_data.maxSupply, float),\n",
        "        'marketCapUsd': _convert(asset_data.marketCapUsd, float),\n",
        "        'volumeUsd24Hr': _convert(asset_data.volumeUsd24Hr, float),\n",
        "        'priceUsd': _convert(asset_data.priceUsd, float),\n",
        "        'changePercent24Hr': _convert(asset_data.changePercent24Hr, float),\n",
        "        'vwap24Hr': _convert(asset_data.vwap24Hr, float),\n",
        "        'explorer': _convert(asset_data.explorer, str)\n",
        "    }\n",
        "\n",
        "    # Add the CoinCap API response timestamp\n",
        "    if coincap_response.timestamp_seconds is not None:\n",
        "        row['coincap_query_time_bq'] = _convertTimestamp(coincap_response.timestamp_seconds)\n",
        "\n",
        "    # Return only non-null values, as BQ handles missing fields\n",
        "    return {k: v for k, v in row.items() if v is not None}\n",
        "\n",
        "# Re-initialize GCP clients (ensuring they are available in this scope)\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Redefine load_gcs_to_bigquery (if it uses any global variables that might be affected)\n",
        "# This function does not directly depend on the CoinCapApi class but is part of the pipeline context.\n",
        "def load_gcs_to_bigquery(gcs_uri, project_id, bq_dataset, bq_table, bq_schema, bq_client_instance):\n",
        "    \"\"\"Loads data from a GCS URI into a BigQuery table.\"\"\"\n",
        "    print(f\"\\nStep: Loading data from GCS into BigQuery...\")\n",
        "    print(f\"  > Source: {gcs_uri}\")\n",
        "    print(f\"  > Target: {bq_dataset}.{bq_table}\")\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
        "    job_config.schema = bq_schema\n",
        "    job_config.autodetect = False\n",
        "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
        "\n",
        "    load_job = bq_client_instance.load_table_from_uri(\n",
        "        gcs_uri,\n",
        "        f\"{project_id}.{bq_dataset}.{bq_table}\",\n",
        "        job_config=job_config\n",
        "    )\n",
        "\n",
        "    print(f\"  > Starting BQ Load Job: {load_job.job_id}\")\n",
        "    load_job.result()\n",
        "    print(f\"  > Job complete. Loaded {load_job.output_rows} rows.\")\n",
        "    print(\"âœ… GCS to BigQuery Load Finished Successfully.\")\n",
        "\n",
        "def run_full_pipeline_without_bq_load():\n",
        "    \"\"\"Executes the API -> GCS pipeline for CoinCap data and returns GCS URI.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # ======================================================\n",
        "        # 1. Download data from CoinCap API\n",
        "        # ======================================================\n",
        "        print(f\"Step 1: Fetching Bitcoin asset data from CoinCap API...\")\n",
        "        api = CoinCapApi()\n",
        "        queryTime = datetime.datetime.now().timestamp()\n",
        "        bitcoin_response = api.get_asset(\"bitcoin\")\n",
        "\n",
        "        records = []\n",
        "        if bitcoin_response and bitcoin_response.data:\n",
        "            record = _convertRow(bitcoin_response)\n",
        "            if record:\n",
        "                records.append(record)\n",
        "\n",
        "        if not records:\n",
        "            print(\"No Bitcoin data found. Exiting.\")\n",
        "            return None # Return None if no records\n",
        "\n",
        "        print(f\"  > Fetched {len(records)} record(s).\")\n",
        "\n",
        "        # ======================================================\n",
        "        # 2. Save data to GCS Bucket\n",
        "        # ======================================================\n",
        "        local_filename = \"bitcoin_data.jsonl\"\n",
        "        with open(local_filename, 'w') as f:\n",
        "            for record in records:\n",
        "                f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "        gcs_filename = f\"{GCS_FOLDER_PATH}/coincap_batch_{int(queryTime)}.jsonl\"\n",
        "\n",
        "        print(f\"\\nStep 2: Uploading data to GCS...\")\n",
        "        print(f\"  > Source: {local_filename}\")\n",
        "        print(f\"  > Destination: gs://{GCS_BUCKET_NAME}/{gcs_filename}\")\n",
        "\n",
        "        bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
        "        blob = bucket.blob(gcs_filename)\n",
        "        blob.upload_from_filename(local_filename)\n",
        "\n",
        "        gcs_uri = f\"gs://{GCS_BUCKET_NAME}/{gcs_filename}\"\n",
        "        print(\"  > Upload complete.\")\n",
        "        print(\"âœ… API to GCS Pipeline Finished Successfully.\")\n",
        "        return gcs_uri # Return the GCS URI\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ ERROR in pipeline: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"âœ… CoinCap API class updated with get_asset_history method.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… CoinCap API class updated with get_asset_history method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e90e5294"
      },
      "source": [
        "## Update Data Fetching for Historical Data\n",
        "\n",
        "### Subtask:\n",
        "Modify the `run_full_pipeline_without_bq_load` function to use the new `get_asset_history` method to fetch historical data for Bitcoin.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b048e6c"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the `run_full_pipeline_without_bq_load` function to fetch historical data using the `get_asset_history` method. I will redefine the cell containing this function, along with `CoinCapApi` and other helper functions, to incorporate the new logic for calculating timestamps, calling `get_asset_history`, and processing its response using a new `TempCoinCapHistoryWrapper` class to adapt historical data points for the existing `_convertRow` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6925ca3",
        "outputId": "0bfeab38-e545-46f1-f075-b487b307e22e"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import datetime\n",
        "import time\n",
        "import requests\n",
        "from collections import defaultdict\n",
        "from google.cloud import storage, bigquery\n",
        "\n",
        "# --- !! CONFIGURE YOUR VARIABLES (re-defined for self-containment) !! ---\n",
        "PROJECT_ID = \"directed-bongo-471119-d1\"\n",
        "GCP_REGION = \"us-central1\"\n",
        "GCS_BUCKET_NAME = \"467_final_3_2\"\n",
        "GCS_FOLDER_PATH = \"data_1\"\n",
        "BQ_DATASET = \"467_final_3_2\"\n",
        "BQ_TABLE = \"data_1_t\"\n",
        "FLIGHT_RECORD_LIMIT = 500 # Not used in this specific CoinCap context, but kept for consistency if pipeline structure changes\n",
        "\n",
        "COINCAP_API_KEY = \"7aaeaf52f203c5f61c86c294210dada652fc5adc8d566875faa9eedebbbdf761\" # Re-define API key\n",
        "\n",
        "# Define BigQuery Schema based on _convertRow output\n",
        "BQ_SCHEMA = [\n",
        "    bigquery.SchemaField(\"id\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"rank\", \"INTEGER\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"symbol\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"name\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"supply\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"maxSupply\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"marketCapUsd\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"volumeUsd24Hr\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"priceUsd\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"changePercent24Hr\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"vwap24Hr\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"explorer\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"coincap_query_time_bq\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
        "]\n",
        "\n",
        "# ==============================================================================\n",
        "# CoinCap API Library Code (Modified to use API Key and add get_asset_history)\n",
        "# ==============================================================================\n",
        "\n",
        "logger = logging.getLogger('coincap_api')\n",
        "logger.addHandler(logging.NullHandler())\n",
        "\n",
        "class CoinCapAsset(object):\n",
        "    \"\"\"Represents a single cryptocurrency asset from the CoinCap API data.\"\"\"\n",
        "    keys = [\"id\", \"rank\", \"symbol\", \"name\", \"supply\", \"maxSupply\",\n",
        "            \"marketCapUsd\", \"volumeUsd24Hr\", \"priceUsd\", \"changePercent24Hr\",\n",
        "            \"vwap24Hr\", \"explorer\"]\n",
        "\n",
        "    def __init__(self, data_dict):\n",
        "        # Initialize attributes from the data_dict\n",
        "        for key in CoinCapAsset.keys:\n",
        "            # Use .get() to safely retrieve values, in case a key is missing\n",
        "            setattr(self, key, data_dict.get(key))\n",
        "\n",
        "class CoinCapResponse(object):\n",
        "    \"\"\"Represents the overall response from the CoinCap API for an asset.\"\"\"\n",
        "    def __init__(self, j):\n",
        "        self.__dict__ = j\n",
        "        # If 'data' exists and is not None, parse it into a CoinCapAsset object\n",
        "        if 'data' in self.__dict__ and self.data is not None:\n",
        "            self.data = CoinCapAsset(self.data)\n",
        "        else:\n",
        "            self.data = None\n",
        "        # Convert timestamp from milliseconds to seconds for general use\n",
        "        if 'timestamp' in self.__dict__ and self.timestamp is not None:\n",
        "            self.timestamp_seconds = int(self.timestamp / 1000)\n",
        "        else:\n",
        "            self.timestamp_seconds = None\n",
        "\n",
        "\n",
        "class CoinCapApi(object):\n",
        "    def __init__(self):\n",
        "        self._api_url = \"https://rest.coincap.io/v3\"\n",
        "        self._last_requests = defaultdict(lambda: 0)\n",
        "        # CoinCap has a rate limit of 2000 requests per minute, so ~0.03 seconds per request\n",
        "        self._rate_limit_seconds = 60 / 2000 # 0.03 seconds per request\n",
        "\n",
        "    def _get_json(self, url_post, callee, params=None):\n",
        "        # Implement a basic rate limit check\n",
        "        time_since_last_request = time.time() - self._last_requests[callee]\n",
        "        if time_since_last_request < self._rate_limit_seconds:\n",
        "            # Wait if we're hitting the rate limit too fast\n",
        "            sleep_time = self._rate_limit_seconds - time_since_last_request\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'application/json',\n",
        "            'Authorization': f'Bearer {COINCAP_API_KEY}'\n",
        "        }\n",
        "\n",
        "        r = requests.get(f\"{self._api_url}{url_post}\", headers=headers, params=params, timeout=60.00)\n",
        "        if r.status_code == 200:\n",
        "            self._last_requests[callee] = time.time()\n",
        "            json_response = r.json()\n",
        "            return json_response\n",
        "        logger.debug(f\"Response not OK. Status {r.status_code} - {r.reason}\")\n",
        "        print(f\"DEBUG: API call to {f'{self._api_url}{url_post}'} failed with status {r.status_code}: {r.reason}. Response text: {r.text}\")\n",
        "        return None\n",
        "\n",
        "    def get_asset(self, asset_id=\"bitcoin\"):\n",
        "        \"\"\"Fetches data for a specific cryptocurrency asset.\"\"\"\n",
        "        json_data = self._get_json(f\"/assets/{asset_id}\", self.get_asset)\n",
        "        return CoinCapResponse(json_data) if json_data else None\n",
        "\n",
        "    def get_asset_history(self, asset_id, interval, start=None, end=None):\n",
        "        \"\"\"Fetches historical data for a specific cryptocurrency asset.\n",
        "\n",
        "        Args:\n",
        "            asset_id (str): The ID of the asset (e.g., \"bitcoin\").\n",
        "            interval (str): The time interval for historical data (e.g., 'd1', 'h1', 'm1').\n",
        "            start (int, optional): Unix timestamp (milliseconds) for the start of the period.\n",
        "            end (int, optional): Unix timestamp (milliseconds) for the end of the period.\n",
        "\n",
        "        Returns:\n",
        "            dict: The JSON response containing historical data, or None if an error occurs.\n",
        "        \"\"\"\n",
        "        url_post = f\"/assets/{asset_id}/history\"\n",
        "        params = {'interval': interval}\n",
        "\n",
        "        if start is not None:\n",
        "            params['start'] = int(start)\n",
        "        if end is not None:\n",
        "            params['end'] = int(end)\n",
        "\n",
        "        json_data = self._get_json(url_post, self.get_asset_history, params=params)\n",
        "        return json_data\n",
        "\n",
        "# ==============================================================================\n",
        "# Data Parser Functions (re-defined to ensure consistency with updated CoinCapApi)\n",
        "# ==============================================================================\n",
        "\n",
        "def _convertTimestamp(timestamp_seconds):\n",
        "    \"\"\"Converts a Unix timestamp (in seconds) to a formatted datetime string.\"\"\"\n",
        "    if timestamp_seconds is not None:\n",
        "        try:\n",
        "            return datetime.datetime.fromtimestamp(timestamp_seconds).strftime('%Y-%m-%d %H:%M:%S')\n",
        "        except Exception:\n",
        "            logger.error(f\"Error converting timestamp {timestamp_seconds}\", exc_info=True)\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "def _convert(data, dataType):\n",
        "    \"\"\"Converts data to the specified dataType, handling None and type conversion errors.\"\"\"\n",
        "    if data is None:\n",
        "        return None\n",
        "    if dataType == str:\n",
        "        # For CoinCap API, some string values might already be clean, but strip() is safe.\n",
        "        return str(data).strip()\n",
        "    try:\n",
        "        return dataType(data)\n",
        "    except Exception:\n",
        "        logger.error(f\"Error converting data '{data}' to {dataType.__name__}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "def _convertRow(coincap_response):\n",
        "    \"\"\"Converts a CoinCapResponse object into a dictionary suitable for BigQuery.\n",
        "    Assumes coincap_response contains a CoinCapAsset object under .data and a timestamp_seconds.\n",
        "    \"\"\"\n",
        "    if coincap_response is None or coincap_response.data is None:\n",
        "        return {}\n",
        "\n",
        "    asset_data = coincap_response.data\n",
        "    row = {\n",
        "        'id': _convert(asset_data.id, str),\n",
        "        'rank': _convert(asset_data.rank, int),\n",
        "        'symbol': _convert(asset_data.symbol, str),\n",
        "        'name': _convert(asset_data.name, str),\n",
        "        'supply': _convert(asset_data.supply, float),\n",
        "        'maxSupply': _convert(asset_data.maxSupply, float),\n",
        "        'marketCapUsd': _convert(asset_data.marketCapUsd, float),\n",
        "        'volumeUsd24Hr': _convert(asset_data.volumeUsd24Hr, float),\n",
        "        'priceUsd': _convert(asset_data.priceUsd, float),\n",
        "        'changePercent24Hr': _convert(asset_data.changePercent24Hr, float),\n",
        "        'vwap24Hr': _convert(asset_data.vwap24Hr, float),\n",
        "        'explorer': _convert(asset_data.explorer, str)\n",
        "    }\n",
        "\n",
        "    # Add the CoinCap API response timestamp\n",
        "    if coincap_response.timestamp_seconds is not None:\n",
        "        row['coincap_query_time_bq'] = _convertTimestamp(coincap_response.timestamp_seconds)\n",
        "\n",
        "    # Return only non-null values, as BQ handles missing fields\n",
        "    return {k: v for k, v in row.items() if v is not None}\n",
        "\n",
        "# Re-initialize GCP clients (ensuring they are available in this scope)\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Redefine load_gcs_to_bigquery (if it uses any global variables that might be affected)\n",
        "# This function does not directly depend on the CoinCapApi class but is part of the pipeline context.\n",
        "def load_gcs_to_bigquery(gcs_uri, project_id, bq_dataset, bq_table, bq_schema, bq_client_instance):\n",
        "    \"\"\"Loads data from a GCS URI into a BigQuery table.\"\"\"\n",
        "    print(f\"\\nStep: Loading data from GCS into BigQuery...\")\n",
        "    print(f\"  > Source: {gcs_uri}\")\n",
        "    print(f\"  > Target: {bq_dataset}.{bq_table}\")\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
        "    job_config.schema = bq_schema\n",
        "    job_config.autodetect = False\n",
        "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
        "\n",
        "    load_job = bq_client_instance.load_table_from_uri(\n",
        "        gcs_uri,\n",
        "        f\"{project_id}.{bq_dataset}.{bq_table}\",\n",
        "        job_config=job_config\n",
        "    )\n",
        "\n",
        "    print(f\"  > Starting BQ Load Job: {load_job.job_id}\")\n",
        "    load_job.result()\n",
        "    print(f\"  > Job complete. Loaded {load_job.output_rows} rows.\")\n",
        "    print(\"âœ… GCS to BigQuery Load Finished Successfully.\")\n",
        "\n",
        "# New wrapper for historical data points to mimic CoinCapResponse structure\n",
        "class TempCoinCapHistoryWrapper:\n",
        "    def __init__(self, data_point):\n",
        "        # CoinCapAsset will only populate keys present in data_point AND CoinCapAsset.keys\n",
        "        # Historical data points usually only have priceUsd, time, date.\n",
        "        # This will set self.data.priceUsd and others will remain None.\n",
        "        self.data = CoinCapAsset(data_point)\n",
        "        # 'time' in historical data is milliseconds Unix timestamp\n",
        "        self.timestamp = data_point.get('time')\n",
        "        self.timestamp_seconds = int(self.timestamp / 1000) if self.timestamp else None\n",
        "\n",
        "def run_full_pipeline_without_bq_load():\n",
        "    \"\"\"Executes the API -> GCS pipeline for CoinCap data and returns GCS URI.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # ======================================================\n",
        "        # 1. Download data from CoinCap API\n",
        "        # ======================================================\n",
        "        print(f\"Step 1: Fetching Bitcoin historical data from CoinCap API...\")\n",
        "        api = CoinCapApi()\n",
        "        queryTime = datetime.datetime.now().timestamp()\n",
        "\n",
        "        # Calculate start and end timestamps for the last 7 days\n",
        "        end_datetime = datetime.datetime.now()\n",
        "        end_timestamp_ms = int(end_datetime.timestamp() * 1000)\n",
        "        start_datetime = end_datetime - datetime.timedelta(days=7)\n",
        "        start_timestamp_ms = int(start_datetime.timestamp() * 1000)\n",
        "\n",
        "        # Fetch historical data\n",
        "        historical_data_response = api.get_asset_history(\n",
        "            asset_id=\"bitcoin\",\n",
        "            interval=\"d1\", # Daily interval\n",
        "            start=start_timestamp_ms,\n",
        "            end=end_timestamp_ms\n",
        "        )\n",
        "\n",
        "        records = []\n",
        "        if historical_data_response and 'data' in historical_data_response:\n",
        "            for data_point in historical_data_response['data']:\n",
        "                # Wrap each historical data point to be compatible with _convertRow\n",
        "                wrapped_response = TempCoinCapHistoryWrapper(data_point)\n",
        "                record = _convertRow(wrapped_response)\n",
        "                if record:\n",
        "                    records.append(record)\n",
        "\n",
        "        if not records:\n",
        "            print(\"No Bitcoin historical data found. Exiting.\")\n",
        "            return None # Return None if no records\n",
        "\n",
        "        print(f\"  > Fetched {len(records)} record(s).\")\n",
        "\n",
        "        # ======================================================\n",
        "        # 2. Save data to GCS Bucket\n",
        "        # ======================================================\n",
        "        local_filename = \"bitcoin_historical_data.jsonl\"\n",
        "        with open(local_filename, 'w') as f:\n",
        "            for record in records:\n",
        "                f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "        gcs_filename = f\"{GCS_FOLDER_PATH}/coincap_history_batch_{int(queryTime)}.jsonl\"\n",
        "\n",
        "        print(f\"\\nStep 2: Uploading data to GCS...\")\n",
        "        print(f\"  > Source: {local_filename}\")\n",
        "        print(f\"  > Destination: gs://{GCS_BUCKET_NAME}/{gcs_filename}\")\n",
        "\n",
        "        bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
        "        blob = bucket.blob(gcs_filename)\n",
        "        blob.upload_from_filename(local_filename)\n",
        "\n",
        "        gcs_uri = f\"gs://{GCS_BUCKET_NAME}/{gcs_filename}\"\n",
        "        print(\"  > Upload complete.\")\n",
        "        print(\"âœ… API to GCS Pipeline Finished Successfully.\")\n",
        "        return gcs_uri # Return the GCS URI\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ ERROR in pipeline: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"âœ… CoinCap API class updated with get_asset_history method, and run_full_pipeline_without_bq_load modified to fetch historical data.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… CoinCap API class updated with get_asset_history method, and run_full_pipeline_without_bq_load modified to fetch historical data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a83d617"
      },
      "source": [
        "**Reasoning**:\n",
        "The `run_full_pipeline_without_bq_load` function has been redefined to fetch historical data using the `get_asset_history` method. I will now execute this function to confirm that it correctly fetches historical Bitcoin data, processes it, and uploads it to GCS.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2979ddbd",
        "outputId": "05cad020-d726-4cef-9213-27da2f9da1f8"
      },
      "source": [
        "gcs_uri_for_historical_data = run_full_pipeline_without_bq_load()\n",
        "print(f\"GCS URI for historical data: {gcs_uri_for_historical_data}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Fetching Bitcoin historical data from CoinCap API...\n",
            "  > Fetched 8 record(s).\n",
            "\n",
            "Step 2: Uploading data to GCS...\n",
            "  > Source: bitcoin_historical_data.jsonl\n",
            "  > Destination: gs://467_final_3_2/data_1/coincap_history_batch_1765634011.jsonl\n",
            "  > Upload complete.\n",
            "âœ… API to GCS Pipeline Finished Successfully.\n",
            "GCS URI for historical data: gs://467_final_3_2/data_1/coincap_history_batch_1765634011.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbd7fcb8"
      },
      "source": [
        "## Adjust Data Conversion for Historical Data\n",
        "\n",
        "### Subtask:\n",
        "Review the structure of the historical data returned by the API and adjust the `_convertRow` function (or create a new one) to correctly parse and format each historical record into a dictionary suitable for BigQuery insertion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03437ec2"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining a new `_convertHistoricalRecord` function and modifying `run_full_pipeline_without_bq_load` to use it for historical data processing. I will redefine the entire cell containing these functions, along with other necessary components like `CoinCapApi` and helper functions, to incorporate these changes and ensure all variables and functions are correctly scoped and updated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d125284",
        "outputId": "e53c6370-7d7e-4a78-f9ed-169dc323af4d"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import datetime\n",
        "import time\n",
        "import requests\n",
        "from collections import defaultdict\n",
        "from google.cloud import storage, bigquery\n",
        "\n",
        "# --- !! CONFIGURE YOUR VARIABLES (re-defined for self-containment) !! ---\n",
        "PROJECT_ID = \"directed-bongo-471119-d1\"\n",
        "GCP_REGION = \"us-central1\"\n",
        "GCS_BUCKET_NAME = \"467_final_3_2\"\n",
        "GCS_FOLDER_PATH = \"data_1\"\n",
        "BQ_DATASET = \"467_final_3_2\"\n",
        "BQ_TABLE = \"data_1_t\"\n",
        "FLIGHT_RECORD_LIMIT = 500 # Not used in this specific CoinCap context, but kept for consistency if pipeline structure changes\n",
        "\n",
        "COINCAP_API_KEY = \"7aaeaf52f203c5f61c86c294210dada652fc5adc8d566875faa9eedebbbdf761\" # Re-define API key\n",
        "\n",
        "# Define BigQuery Schema based on _convertRow output\n",
        "BQ_SCHEMA = [\n",
        "    bigquery.SchemaField(\"id\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"rank\", \"INTEGER\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"symbol\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"name\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"supply\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"maxSupply\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"marketCapUsd\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"volumeUsd24Hr\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"priceUsd\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"changePercent24Hr\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"vwap24Hr\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"explorer\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"coincap_query_time_bq\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
        "]\n",
        "\n",
        "# ==============================================================================\n",
        "# CoinCap API Library Code (Modified to use API Key and add get_asset_history)\n",
        "# ==============================================================================\n",
        "\n",
        "logger = logging.getLogger('coincap_api')\n",
        "logger.addHandler(logging.NullHandler())\n",
        "\n",
        "class CoinCapAsset(object):\n",
        "    \"\"\"Represents a single cryptocurrency asset from the CoinCap API data.\"\"\"\n",
        "    keys = [\"id\", \"rank\", \"symbol\", \"name\", \"supply\", \"maxSupply\",\n",
        "            \"marketCapUsd\", \"volumeUsd24Hr\", \"priceUsd\", \"changePercent24Hr\",\n",
        "            \"vwap24Hr\", \"explorer\"]\n",
        "\n",
        "    def __init__(self, data_dict):\n",
        "        # Initialize attributes from the data_dict\n",
        "        for key in CoinCapAsset.keys:\n",
        "            # Use .get() to safely retrieve values, in case a key is missing\n",
        "            setattr(self, key, data_dict.get(key))\n",
        "\n",
        "class CoinCapResponse(object):\n",
        "    \"\"\"Represents the overall response from the CoinCap API for an asset.\"\"\"\n",
        "    def __init__(self, j):\n",
        "        self.__dict__ = j\n",
        "        # If 'data' exists and is not None, parse it into a CoinCapAsset object\n",
        "        if 'data' in self.__dict__ and self.data is not None:\n",
        "            self.data = CoinCapAsset(self.data)\n",
        "        else:\n",
        "            self.data = None\n",
        "        # Convert timestamp from milliseconds to seconds for general use\n",
        "        if 'timestamp' in self.__dict__ and self.timestamp is not None:\n",
        "            self.timestamp_seconds = int(self.timestamp / 1000)\n",
        "        else:\n",
        "            self.timestamp_seconds = None\n",
        "\n",
        "\n",
        "class CoinCapApi(object):\n",
        "    def __init__(self):\n",
        "        self._api_url = \"https://rest.coincap.io/v3\"\n",
        "        self._last_requests = defaultdict(lambda: 0)\n",
        "        # CoinCap has a rate limit of 2000 requests per minute, so ~0.03 seconds per request\n",
        "        self._rate_limit_seconds = 60 / 2000 # 0.03 seconds per request\n",
        "\n",
        "    def _get_json(self, url_post, callee, params=None):\n",
        "        # Implement a basic rate limit check\n",
        "        time_since_last_request = time.time() - self._last_requests[callee]\n",
        "        if time_since_last_request < self._rate_limit_seconds:\n",
        "            # Wait if we're hitting the rate limit too fast\n",
        "            sleep_time = self._rate_limit_seconds - time_since_last_request\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'application/json',\n",
        "            'Authorization': f'Bearer {COINCAP_API_KEY}'\n",
        "        }\n",
        "\n",
        "        r = requests.get(f\"{self._api_url}{url_post}\", headers=headers, params=params, timeout=60.00)\n",
        "        if r.status_code == 200:\n",
        "            self._last_requests[callee] = time.time()\n",
        "            json_response = r.json()\n",
        "            return json_response\n",
        "        logger.debug(f\"Response not OK. Status {r.status_code} - {r.reason}\")\n",
        "        print(f\"DEBUG: API call to {f'{self._api_url}{url_post}'} failed with status {r.status_code}: {r.reason}. Response text: {r.text}\")\n",
        "        return None\n",
        "\n",
        "    def get_asset(self, asset_id=\"bitcoin\"):\n",
        "        \"\"\"Fetches data for a specific cryptocurrency asset.\"\"\"\n",
        "        json_data = self._get_json(f\"/assets/{asset_id}\", self.get_asset)\n",
        "        return CoinCapResponse(json_data) if json_data else None\n",
        "\n",
        "    def get_asset_history(self, asset_id, interval, start=None, end=None):\n",
        "        \"\"\"Fetches historical data for a specific cryptocurrency asset.\n",
        "\n",
        "        Args:\n",
        "            asset_id (str): The ID of the asset (e.g., \"bitcoin\").\n",
        "            interval (str): The time interval for historical data (e.g., 'd1', 'h1', 'm1').\n",
        "            start (int, optional): Unix timestamp (milliseconds) for the start of the period.\n",
        "            end (int, optional): Unix timestamp (milliseconds) for the end of the period.\n",
        "\n",
        "        Returns:\n",
        "            dict: The JSON response containing historical data, or None if an error occurs.\n",
        "        \"\"\"\n",
        "        url_post = f\"/assets/{asset_id}/history\"\n",
        "        params = {'interval': interval}\n",
        "\n",
        "        if start is not None:\n",
        "            params['start'] = int(start)\n",
        "        if end is not None:\n",
        "            params['end'] = int(end)\n",
        "\n",
        "        json_data = self._get_json(url_post, self.get_asset_history, params=params)\n",
        "        return json_data\n",
        "\n",
        "# ==============================================================================\n",
        "# Data Parser Functions\n",
        "# ==============================================================================\n",
        "\n",
        "def _convertTimestamp(timestamp_seconds):\n",
        "    \"\"\"Converts a Unix timestamp (in seconds) to a formatted datetime string.\"\"\"\n",
        "    if timestamp_seconds is not None:\n",
        "        try:\n",
        "            return datetime.datetime.fromtimestamp(timestamp_seconds).strftime('%Y-%m-%d %H:%M:%S')\n",
        "        except Exception:\n",
        "            logger.error(f\"Error converting timestamp {timestamp_seconds}\", exc_info=True)\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "def _convert(data, dataType):\n",
        "    \"\"\"Converts data to the specified dataType, handling None and type conversion errors.\"\"\"\n",
        "    if data is None:\n",
        "        return None\n",
        "    if dataType == str:\n",
        "        # For CoinCap API, some string values might already be clean, but strip() is safe.\n",
        "        return str(data).strip()\n",
        "    try:\n",
        "        return dataType(data)\n",
        "    except Exception:\n",
        "        logger.error(f\"Error converting data '{data}' to {dataType.__name__}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "def _convertRow(coincap_response):\n",
        "    \"\"\"Converts a CoinCapResponse object into a dictionary suitable for BigQuery.\n",
        "    Assumes coincap_response contains a CoinCapAsset object under .data and a timestamp_seconds.\n",
        "    \"\"\"\n",
        "    if coincap_response is None or coincap_response.data is None:\n",
        "        return {}\n",
        "\n",
        "    asset_data = coincap_response.data\n",
        "    row = {\n",
        "        'id': _convert(asset_data.id, str),\n",
        "        'rank': _convert(asset_data.rank, int),\n",
        "        'symbol': _convert(asset_data.symbol, str),\n",
        "        'name': _convert(asset_data.name, str),\n",
        "        'supply': _convert(asset_data.supply, float),\n",
        "        'maxSupply': _convert(asset_data.maxSupply, float),\n",
        "        'marketCapUsd': _convert(asset_data.marketCapUsd, float),\n",
        "        'volumeUsd24Hr': _convert(asset_data.volumeUsd24Hr, float),\n",
        "        'priceUsd': _convert(asset_data.priceUsd, float),\n",
        "        'changePercent24Hr': _convert(asset_data.changePercent24Hr, float),\n",
        "        'vwap24Hr': _convert(asset_data.vwap24Hr, float),\n",
        "        'explorer': _convert(asset_data.explorer, str)\n",
        "    }\n",
        "\n",
        "    # Add the CoinCap API response timestamp\n",
        "    if coincap_response.timestamp_seconds is not None:\n",
        "        row['coincap_query_time_bq'] = _convertTimestamp(coincap_response.timestamp_seconds)\n",
        "\n",
        "    # Return only non-null values, as BQ handles missing fields\n",
        "    return {k: v for k, v in row.items() if v is not None}\n",
        "\n",
        "def _convertHistoricalRecord(historical_data_point, asset_id):\n",
        "    \"\"\"Converts a single historical data point into a dictionary suitable for BigQuery.\n",
        "\n",
        "    Args:\n",
        "        historical_data_point (dict): A dictionary representing a single historical entry from the CoinCap API response.\n",
        "                                      Expected keys: 'priceUsd', 'time', and potentially others like 'volume'.\n",
        "        asset_id (str): The ID of the asset (e.g., \"bitcoin\").\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the formatted historical record, or an empty dict if conversion fails.\n",
        "    \"\"\"\n",
        "    if not historical_data_point:\n",
        "        return {}\n",
        "\n",
        "    # Extract and convert priceUsd\n",
        "    price_usd = _convert(historical_data_point.get('priceUsd'), float)\n",
        "\n",
        "    # Extract and convert timestamp from milliseconds to formatted datetime string\n",
        "    timestamp_ms = historical_data_point.get('time')\n",
        "    coincap_query_time_bq = None\n",
        "    if timestamp_ms is not None:\n",
        "        coincap_query_time_bq = _convertTimestamp(timestamp_ms / 1000)\n",
        "\n",
        "    # Construct the record dictionary\n",
        "    record = {\n",
        "        'id': asset_id,\n",
        "        'priceUsd': price_usd,\n",
        "        'coincap_query_time_bq': coincap_query_time_bq\n",
        "    }\n",
        "\n",
        "    # Optionally add other fields if available in historical_data_point and relevant\n",
        "    # For instance, if 'volume' is present and we want to include it:\n",
        "    # record['volumeUsd24Hr'] = _convert(historical_data_point.get('volume'), float)\n",
        "\n",
        "    # Return only non-null values\n",
        "    return {k: v for k, v in record.items() if v is not None}\n",
        "\n",
        "# Re-initialize GCP clients (ensuring they are available in this scope)\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Redefine load_gcs_to_bigquery (if it uses any global variables that might be affected)\n",
        "# This function does not directly depend on the CoinCapApi class but is part of the pipeline context.\n",
        "def load_gcs_to_bigquery(gcs_uri, project_id, bq_dataset, bq_table, bq_schema, bq_client_instance):\n",
        "    \"\"\"Loads data from a GCS URI into a BigQuery table.\"\"\"\n",
        "    print(f\"\\nStep: Loading data from GCS into BigQuery...\")\n",
        "    print(f\"  > Source: {gcs_uri}\")\n",
        "    print(f\"  > Target: {bq_dataset}.{bq_table}\")\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
        "    job_config.schema = bq_schema\n",
        "    job_config.autodetect = False\n",
        "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
        "\n",
        "    load_job = bq_client_instance.load_table_from_uri(\n",
        "        gcs_uri,\n",
        "        f\"{project_id}.{bq_dataset}.{bq_table}\",\n",
        "        job_config=job_config\n",
        "    )\n",
        "\n",
        "    print(f\"  > Starting BQ Load Job: {load_job.job_id}\")\n",
        "    load_job.result()\n",
        "    print(f\"  > Job complete. Loaded {load_job.output_rows} rows.\")\n",
        "    print(\"âœ… GCS to BigQuery Load Finished Successfully.\")\n",
        "\n",
        "def run_full_pipeline_without_bq_load():\n",
        "    \"\"\"Executes the API -> GCS pipeline for CoinCap data and returns GCS URI.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # ======================================================\n",
        "        # 1. Download data from CoinCap API\n",
        "        # ======================================================\n",
        "        print(f\"Step 1: Fetching Bitcoin historical data from CoinCap API...\")\n",
        "        api = CoinCapApi()\n",
        "        queryTime = datetime.datetime.now().timestamp()\n",
        "\n",
        "        # Define asset_id for historical data fetch\n",
        "        asset_id_to_fetch = \"bitcoin\"\n",
        "\n",
        "        # Calculate start and end timestamps for the last 7 days\n",
        "        end_datetime = datetime.datetime.now()\n",
        "        end_timestamp_ms = int(end_datetime.timestamp() * 1000)\n",
        "        start_datetime = end_datetime - datetime.timedelta(days=7)\n",
        "        start_timestamp_ms = int(start_datetime.timestamp() * 1000)\n",
        "\n",
        "        # Fetch historical data\n",
        "        historical_data_response = api.get_asset_history(\n",
        "            asset_id=asset_id_to_fetch,\n",
        "            interval=\"d1\", # Daily interval\n",
        "            start=start_timestamp_ms,\n",
        "            end=end_timestamp_ms\n",
        "        )\n",
        "\n",
        "        records = []\n",
        "        if historical_data_response and 'data' in historical_data_response:\n",
        "            for data_point in historical_data_response['data']:\n",
        "                # Use the new _convertHistoricalRecord function\n",
        "                record = _convertHistoricalRecord(data_point, asset_id_to_fetch)\n",
        "                if record:\n",
        "                    records.append(record)\n",
        "\n",
        "        if not records:\n",
        "            print(\"No Bitcoin historical data found. Exiting.\")\n",
        "            return None # Return None if no records\n",
        "\n",
        "        print(f\"  > Fetched {len(records)} record(s).\")\n",
        "\n",
        "        # ======================================================\n",
        "        # 2. Save data to GCS Bucket\n",
        "        # ======================================================\n",
        "        local_filename = \"bitcoin_historical_data.jsonl\"\n",
        "        with open(local_filename, 'w') as f:\n",
        "            for record in records:\n",
        "                f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "        gcs_filename = f\"{GCS_FOLDER_PATH}/coincap_history_batch_{int(queryTime)}.jsonl\"\n",
        "\n",
        "        print(f\"\\nStep 2: Uploading data to GCS...\")\n",
        "        print(f\"  > Source: {local_filename}\")\n",
        "        print(f\"  > Destination: gs://{GCS_BUCKET_NAME}/{gcs_filename}\")\n",
        "\n",
        "        bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
        "        blob = bucket.blob(gcs_filename)\n",
        "        blob.upload_from_filename(local_filename)\n",
        "\n",
        "        gcs_uri = f\"gs://{GCS_BUCKET_NAME}/{gcs_filename}\"\n",
        "        print(\"  > Upload complete.\")\n",
        "        print(\"âœ… API to GCS Pipeline Finished Successfully.\")\n",
        "        return gcs_uri # Return the GCS URI\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ ERROR in pipeline: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"âœ… CoinCap API class and pipeline functions redefined with _convertHistoricalRecord and updated run_full_pipeline_without_bq_load.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… CoinCap API class and pipeline functions redefined with _convertHistoricalRecord and updated run_full_pipeline_without_bq_load.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1da6109c"
      },
      "source": [
        "**Reasoning**:\n",
        "The `run_full_pipeline_without_bq_load` function has been redefined to use the new `_convertHistoricalRecord` function for historical data. I will now execute this function to confirm that it correctly fetches historical Bitcoin data, processes it, and uploads it to GCS with the updated conversion logic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c406a3de",
        "outputId": "bbfcdbab-8e7b-47be-f42a-0f931c0bc559"
      },
      "source": [
        "gcs_uri_for_historical_data = run_full_pipeline_without_bq_load()\n",
        "print(f\"GCS URI for historical data: {gcs_uri_for_historical_data}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Fetching Bitcoin historical data from CoinCap API...\n",
            "  > Fetched 8 record(s).\n",
            "\n",
            "Step 2: Uploading data to GCS...\n",
            "  > Source: bitcoin_historical_data.jsonl\n",
            "  > Destination: gs://467_final_3_2/data_1/coincap_history_batch_1765634071.jsonl\n",
            "  > Upload complete.\n",
            "âœ… API to GCS Pipeline Finished Successfully.\n",
            "GCS URI for historical data: gs://467_final_3_2/data_1/coincap_history_batch_1765634071.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7106154"
      },
      "source": [
        "## Update BigQuery Schema\n",
        "\n",
        "### Subtask:\n",
        "Review and update the `BQ_SCHEMA` to accommodate any new fields or different data types that might be present in the historical data, ensuring compatibility with BigQuery.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eea51ad"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires updating the `BQ_SCHEMA` to reflect the fields generated by `_convertHistoricalRecord`. I will redefine the code cell to modify the `BQ_SCHEMA` to only include `id` (STRING), `priceUsd` (FLOAT), and `coincap_query_time_bq` (TIMESTAMP) and ensure the mode is `NULLABLE` for all.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfa9a4a9",
        "outputId": "cbd7882c-d26f-41e1-98ea-6e4decb70add"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import datetime\n",
        "import time\n",
        "import requests\n",
        "from collections import defaultdict\n",
        "from google.cloud import storage, bigquery\n",
        "\n",
        "# --- !! CONFIGURE YOUR VARIABLES (re-defined for self-containment) !! ---\n",
        "PROJECT_ID = \"directed-bongo-471119-d1\"\n",
        "GCP_REGION = \"us-central1\"\n",
        "GCS_BUCKET_NAME = \"467_final_3_2\"\n",
        "GCS_FOLDER_PATH = \"data_1\"\n",
        "BQ_DATASET = \"467_final_3_2\"\n",
        "BQ_TABLE = \"data_1_t\"\n",
        "FLIGHT_RECORD_LIMIT = 500 # Not used in this specific CoinCap context, but kept for consistency if pipeline structure changes\n",
        "\n",
        "COINCAP_API_KEY = \"7aaeaf52f203c5f61c86c294210dada652fc5adc8d566875faa9eedebbbdf761\" # Re-define API key\n",
        "\n",
        "# Define BigQuery Schema based on _convertHistoricalRecord output\n",
        "BQ_SCHEMA = [\n",
        "    bigquery.SchemaField(\"id\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"priceUsd\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"coincap_query_time_bq\", \"TIMESTAMP\", mode=\"NULLABLE\")\n",
        "]\n",
        "\n",
        "# ==============================================================================\n",
        "# CoinCap API Library Code (Modified to use API Key and add get_asset_history)\n",
        "# ==============================================================================\n",
        "\n",
        "logger = logging.getLogger('coincap_api')\n",
        "logger.addHandler(logging.NullHandler())\n",
        "\n",
        "class CoinCapAsset(object):\n",
        "    \"\"\"Represents a single cryptocurrency asset from the CoinCap API data.\"\"\"\n",
        "    keys = [\"id\", \"rank\", \"symbol\", \"name\", \"supply\", \"maxSupply\",\n",
        "            \"marketCapUsd\", \"volumeUsd24Hr\", \"priceUsd\", \"changePercent24Hr\",\n",
        "            \"vwap24Hr\", \"explorer\"]\n",
        "\n",
        "    def __init__(self, data_dict):\n",
        "        # Initialize attributes from the data_dict\n",
        "        for key in CoinCapAsset.keys:\n",
        "            # Use .get() to safely retrieve values, in case a key is missing\n",
        "            setattr(self, key, data_dict.get(key))\n",
        "\n",
        "class CoinCapResponse(object):\n",
        "    \"\"\"Represents the overall response from the CoinCap API for an asset.\"\"\"\n",
        "    def __init__(self, j):\n",
        "        self.__dict__ = j\n",
        "        # If 'data' exists and is not None, parse it into a CoinCapAsset object\n",
        "        if 'data' in self.__dict__ and self.data is not None:\n",
        "            self.data = CoinCapAsset(self.data)\n",
        "        else:\n",
        "            self.data = None\n",
        "        # Convert timestamp from milliseconds to seconds for general use\n",
        "        if 'timestamp' in self.__dict__ and self.timestamp is not None:\n",
        "            self.timestamp_seconds = int(self.timestamp / 1000)\n",
        "        else:\n",
        "            self.timestamp_seconds = None\n",
        "\n",
        "\n",
        "class CoinCapApi(object):\n",
        "    def __init__(self):\n",
        "        self._api_url = \"https://rest.coincap.io/v3\"\n",
        "        self._last_requests = defaultdict(lambda: 0)\n",
        "        # CoinCap has a rate limit of 2000 requests per minute, so ~0.03 seconds per request\n",
        "        self._rate_limit_seconds = 60 / 2000 # 0.03 seconds per request\n",
        "\n",
        "    def _get_json(self, url_post, callee, params=None):\n",
        "        # Implement a basic rate limit check\n",
        "        time_since_last_request = time.time() - self._last_requests[callee]\n",
        "        if time_since_last_request < self._rate_limit_seconds:\n",
        "            # Wait if we're hitting the rate limit too fast\n",
        "            sleep_time = self._rate_limit_seconds - time_since_last_request\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'application/json',\n",
        "            'Authorization': f'Bearer {COINCAP_API_KEY}'\n",
        "        }\n",
        "\n",
        "        r = requests.get(f\"{self._api_url}{url_post}\", headers=headers, params=params, timeout=60.00)\n",
        "        if r.status_code == 200:\n",
        "            self._last_requests[callee] = time.time()\n",
        "            json_response = r.json()\n",
        "            return json_response\n",
        "        logger.debug(f\"Response not OK. Status {r.status_code} - {r.reason}\")\n",
        "        print(f\"DEBUG: API call to {f'{self._api_url}{url_post}'} failed with status {r.status_code}: {r.reason}. Response text: {r.text}\")\n",
        "        return None\n",
        "\n",
        "    def get_asset(self, asset_id=\"bitcoin\"):\n",
        "        \"\"\"Fetches data for a specific cryptocurrency asset.\"\"\"\n",
        "        json_data = self._get_json(f\"/assets/{asset_id}\", self.get_asset)\n",
        "        return CoinCapResponse(json_data) if json_data else None\n",
        "\n",
        "    def get_asset_history(self, asset_id, interval, start=None, end=None):\n",
        "        \"\"\"Fetches historical data for a specific cryptocurrency asset.\n",
        "\n",
        "        Args:\n",
        "            asset_id (str): The ID of the asset (e.g., \"bitcoin\").\n",
        "            interval (str): The time interval for historical data (e.g., 'd1', 'h1', 'm1').\n",
        "            start (int, optional): Unix timestamp (milliseconds) for the start of the period.\n",
        "            end (int, optional): Unix timestamp (milliseconds) for the end of the period.\n",
        "\n",
        "        Returns:\n",
        "            dict: The JSON response containing historical data, or None if an error occurs.\n",
        "        \"\"\"\n",
        "        url_post = f\"/assets/{asset_id}/history\"\n",
        "        params = {'interval': interval}\n",
        "\n",
        "        if start is not None:\n",
        "            params['start'] = int(start)\n",
        "        if end is not None:\n",
        "            params['end'] = int(end)\n",
        "\n",
        "        json_data = self._get_json(url_post, self.get_asset_history, params=params)\n",
        "        return json_data\n",
        "\n",
        "# ==============================================================================\n",
        "# Data Parser Functions\n",
        "# ==============================================================================\n",
        "\n",
        "def _convertTimestamp(timestamp_seconds):\n",
        "    \"\"\"Converts a Unix timestamp (in seconds) to a formatted datetime string.\"\"\"\n",
        "    if timestamp_seconds is not None:\n",
        "        try:\n",
        "            return datetime.datetime.fromtimestamp(timestamp_seconds).strftime('%Y-%m-%d %H:%M:%S')\n",
        "        except Exception:\n",
        "            logger.error(f\"Error converting timestamp {timestamp_seconds}\", exc_info=True)\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "def _convert(data, dataType):\n",
        "    \"\"\"Converts data to the specified dataType, handling None and type conversion errors.\"\"\"\n",
        "    if data is None:\n",
        "        return None\n",
        "    if dataType == str:\n",
        "        # For CoinCap API, some string values might already be clean, but strip() is safe.\n",
        "        return str(data).strip()\n",
        "    try:\n",
        "        return dataType(data)\n",
        "    except Exception:\n",
        "        logger.error(f\"Error converting data '{data}' to {dataType.__name__}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "def _convertRow(coincap_response):\n",
        "    \"\"\"Converts a CoinCapResponse object into a dictionary suitable for BigQuery.\n",
        "    Assumes coincap_response contains a CoinCapAsset object under .data and a timestamp_seconds.\n",
        "    \"\"\"\n",
        "    if coincap_response is None or coincap_response.data is None:\n",
        "        return {}\n",
        "\n",
        "    asset_data = coincap_response.data\n",
        "    row = {\n",
        "        'id': _convert(asset_data.id, str),\n",
        "        'rank': _convert(asset_data.rank, int),\n",
        "        'symbol': _convert(asset_data.symbol, str),\n",
        "        'name': _convert(asset_data.name, str),\n",
        "        'supply': _convert(asset_data.supply, float),\n",
        "        'maxSupply': _convert(asset_data.maxSupply, float),\n",
        "        'marketCapUsd': _convert(asset_data.marketCapUsd, float),\n",
        "        'volumeUsd24Hr': _convert(asset_data.volumeUsd24Hr, float),\n",
        "        'priceUsd': _convert(asset_data.priceUsd, float),\n",
        "        'changePercent24Hr': _convert(asset_data.changePercent24Hr, float),\n",
        "        'vwap24Hr': _convert(asset_data.vwap24Hr, float),\n",
        "        'explorer': _convert(asset_data.explorer, str)\n",
        "    }\n",
        "\n",
        "    # Add the CoinCap API response timestamp\n",
        "    if coincap_response.timestamp_seconds is not None:\n",
        "        row['coincap_query_time_bq'] = _convertTimestamp(coincap_response.timestamp_seconds)\n",
        "\n",
        "    # Return only non-null values, as BQ handles missing fields\n",
        "    return {k: v for k, v in row.items() if v is not None}\n",
        "\n",
        "def _convertHistoricalRecord(historical_data_point, asset_id):\n",
        "    \"\"\"Converts a single historical data point into a dictionary suitable for BigQuery.\n",
        "\n",
        "    Args:\n",
        "        historical_data_point (dict): A dictionary representing a single historical entry from the CoinCap API response.\n",
        "                                      Expected keys: 'priceUsd', 'time', and potentially others like 'volume'.\n",
        "        asset_id (str): The ID of the asset (e.g., \"bitcoin\").\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the formatted historical record, or an empty dict if conversion fails.\n",
        "    \"\"\"\n",
        "    if not historical_data_point:\n",
        "        return {}\n",
        "\n",
        "    # Extract and convert priceUsd\n",
        "    price_usd = _convert(historical_data_point.get('priceUsd'), float)\n",
        "\n",
        "    # Extract and convert timestamp from milliseconds to formatted datetime string\n",
        "    timestamp_ms = historical_data_point.get('time')\n",
        "    coincap_query_time_bq = None\n",
        "    if timestamp_ms is not None:\n",
        "        coincap_query_time_bq = _convertTimestamp(timestamp_ms / 1000)\n",
        "\n",
        "    # Construct the record dictionary\n",
        "    record = {\n",
        "        'id': asset_id,\n",
        "        'priceUsd': price_usd,\n",
        "        'coincap_query_time_bq': coincap_query_time_bq\n",
        "    }\n",
        "\n",
        "    # Optionally add other fields if available in historical_data_point and relevant\n",
        "    # For instance, if 'volume' is present and we want to include it:\n",
        "    # record['volumeUsd24Hr'] = _convert(historical_data_point.get('volume'), float)\n",
        "\n",
        "    # Return only non-null values\n",
        "    return {k: v for k, v in record.items() if v is not None}\n",
        "\n",
        "# Re-initialize GCP clients (ensuring they are available in this scope)\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Redefine load_gcs_to_bigquery (if it uses any global variables that might be affected)\n",
        "# This function does not directly depend on the CoinCapApi class but is part of the pipeline context.\n",
        "def load_gcs_to_bigquery(gcs_uri, project_id, bq_dataset, bq_table, bq_schema, bq_client_instance):\n",
        "    \"\"\"Loads data from a GCS URI into a BigQuery table.\"\"\"\n",
        "    print(f\"\\nStep: Loading data from GCS into BigQuery...\")\n",
        "    print(f\"  > Source: {gcs_uri}\")\n",
        "    print(f\"  > Target: {bq_dataset}.{bq_table}\")\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
        "    job_config.schema = bq_schema\n",
        "    job_config.autodetect = False\n",
        "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
        "\n",
        "    load_job = bq_client_instance.load_table_from_uri(\n",
        "        gcs_uri,\n",
        "        f\"{project_id}.{bq_dataset}.{bq_table}\",\n",
        "        job_config=job_config\n",
        "    )\n",
        "\n",
        "    print(f\"  > Starting BQ Load Job: {load_job.job_id}\")\n",
        "    load_job.result()\n",
        "    print(f\"  > Job complete. Loaded {load_job.output_rows} rows.\")\n",
        "    print(\"âœ… GCS to BigQuery Load Finished Successfully.\")\n",
        "\n",
        "def run_full_pipeline_without_bq_load():\n",
        "    \"\"\"Executes the API -> GCS pipeline for CoinCap data and returns GCS URI.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # ======================================================\n",
        "        # 1. Download data from CoinCap API\n",
        "        # ======================================================\n",
        "        print(f\"Step 1: Fetching Bitcoin historical data from CoinCap API...\")\n",
        "        api = CoinCapApi()\n",
        "        queryTime = datetime.datetime.now().timestamp()\n",
        "\n",
        "        # Define asset_id for historical data fetch\n",
        "        asset_id_to_fetch = \"bitcoin\"\n",
        "\n",
        "        # Calculate start and end timestamps for the last 7 days\n",
        "        end_datetime = datetime.datetime.now()\n",
        "        end_timestamp_ms = int(end_datetime.timestamp() * 1000)\n",
        "        start_datetime = end_datetime - datetime.timedelta(days=7)\n",
        "        start_timestamp_ms = int(start_datetime.timestamp() * 1000)\n",
        "\n",
        "        # Fetch historical data\n",
        "        historical_data_response = api.get_asset_history(\n",
        "            asset_id=asset_id_to_fetch,\n",
        "            interval=\"d1\", # Daily interval\n",
        "            start=start_timestamp_ms,\n",
        "            end=end_timestamp_ms\n",
        "        )\n",
        "\n",
        "        records = []\n",
        "        if historical_data_response and 'data' in historical_data_response:\n",
        "            for data_point in historical_data_response['data']:\n",
        "                # Use the new _convertHistoricalRecord function\n",
        "                record = _convertHistoricalRecord(data_point, asset_id_to_fetch)\n",
        "                if record:\n",
        "                    records.append(record)\n",
        "\n",
        "        if not records:\n",
        "            print(\"No Bitcoin historical data found. Exiting.\")\n",
        "            return None # Return None if no records\n",
        "\n",
        "        print(f\"  > Fetched {len(records)} record(s).\")\n",
        "\n",
        "        # ======================================================\n",
        "        # 2. Save data to GCS Bucket\n",
        "        # ======================================================\n",
        "        local_filename = \"bitcoin_historical_data.jsonl\"\n",
        "        with open(local_filename, 'w') as f:\n",
        "            for record in records:\n",
        "                f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "        gcs_filename = f\"{GCS_FOLDER_PATH}/coincap_history_batch_{int(queryTime)}.jsonl\"\n",
        "\n",
        "        print(f\"\\nStep 2: Uploading data to GCS...\")\n",
        "        print(f\"  > Source: {local_filename}\")\n",
        "        print(f\"  > Destination: gs://{GCS_BUCKET_NAME}/{gcs_filename}\")\n",
        "\n",
        "        bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
        "        blob = bucket.blob(gcs_filename)\n",
        "        blob.upload_from_filename(local_filename)\n",
        "\n",
        "        gcs_uri = f\"gs://{GCS_BUCKET_NAME}/{gcs_filename}\"\n",
        "        print(\"  > Upload complete.\")\n",
        "        print(\"âœ… API to GCS Pipeline Finished Successfully.\")\n",
        "        return gcs_uri # Return the GCS URI\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ ERROR in pipeline: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"âœ… CoinCap API class and pipeline functions redefined with _convertHistoricalRecord and updated run_full_pipeline_without_bq_load.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… CoinCap API class and pipeline functions redefined with _convertHistoricalRecord and updated run_full_pipeline_without_bq_load.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de172bb9"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to update the `BQ_SCHEMA` to match the output of the `_convertHistoricalRecord` function. I have redefined the `BQ_SCHEMA` list to include only `id`, `priceUsd`, and `coincap_query_time_bq` with their appropriate data types and `NULLABLE` mode. The rest of the code in the cell remains the same to maintain self-containment and consistency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78c4dbf1",
        "outputId": "505bba87-6de2-49a7-e8b7-76d0f52a4f5c"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import datetime\n",
        "import time\n",
        "import requests\n",
        "from collections import defaultdict\n",
        "from google.cloud import storage, bigquery\n",
        "\n",
        "# --- !! CONFIGURE YOUR VARIABLES (re-defined for self-containment) !! ---\n",
        "PROJECT_ID = \"directed-bongo-471119-d1\"\n",
        "GCP_REGION = \"us-central1\"\n",
        "GCS_BUCKET_NAME = \"467_final_3_2\"\n",
        "GCS_FOLDER_PATH = \"data_1\"\n",
        "BQ_DATASET = \"467_final_3_2\"\n",
        "BQ_TABLE = \"data_1_t\"\n",
        "FLIGHT_RECORD_LIMIT = 500 # Not used in this specific CoinCap context, but kept for consistency if pipeline structure changes\n",
        "\n",
        "COINCAP_API_KEY = \"7aaeaf52f203c5f61c86c294210dada652fc5adc8d566875faa9eedebbbdf761\" # Re-define API key\n",
        "\n",
        "# Define BigQuery Schema based on _convertHistoricalRecord output\n",
        "BQ_SCHEMA = [\n",
        "    bigquery.SchemaField(\"id\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"priceUsd\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"coincap_query_time_bq\", \"TIMESTAMP\", mode=\"NULLABLE\")\n",
        "]\n",
        "\n",
        "# ==============================================================================\n",
        "# CoinCap API Library Code (Modified to use API Key and add get_asset_history)\n",
        "# ==============================================================================\n",
        "\n",
        "logger = logging.getLogger('coincap_api')\n",
        "logger.addHandler(logging.NullHandler())\n",
        "\n",
        "class CoinCapAsset(object):\n",
        "    \"\"\"Represents a single cryptocurrency asset from the CoinCap API data.\"\"\"\n",
        "    keys = [\"id\", \"rank\", \"symbol\", \"name\", \"supply\", \"maxSupply\",\n",
        "            \"marketCapUsd\", \"volumeUsd24Hr\", \"priceUsd\", \"changePercent24Hr\",\n",
        "            \"vwap24Hr\", \"explorer\"]\n",
        "\n",
        "    def __init__(self, data_dict):\n",
        "        # Initialize attributes from the data_dict\n",
        "        for key in CoinCapAsset.keys:\n",
        "            # Use .get() to safely retrieve values, in case a key is missing\n",
        "            setattr(self, key, data_dict.get(key))\n",
        "\n",
        "class CoinCapResponse(object):\n",
        "    \"\"\"Represents the overall response from the CoinCap API for an asset.\"\"\"\n",
        "    def __init__(self, j):\n",
        "        self.__dict__ = j\n",
        "        # If 'data' exists and is not None, parse it into a CoinCapAsset object\n",
        "        if 'data' in self.__dict__ and self.data is not None:\n",
        "            self.data = CoinCapAsset(self.data)\n",
        "        else:\n",
        "            self.data = None\n",
        "        # Convert timestamp from milliseconds to seconds for general use\n",
        "        if 'timestamp' in self.__dict__ and self.timestamp is not None:\n",
        "            self.timestamp_seconds = int(self.timestamp / 1000)\n",
        "        else:\n",
        "            self.timestamp_seconds = None\n",
        "\n",
        "\n",
        "class CoinCapApi(object):\n",
        "    def __init__(self):\n",
        "        self._api_url = \"https://rest.coincap.io/v3\"\n",
        "        self._last_requests = defaultdict(lambda: 0)\n",
        "        # CoinCap has a rate limit of 2000 requests per minute, so ~0.03 seconds per request\n",
        "        self._rate_limit_seconds = 60 / 2000 # 0.03 seconds per request\n",
        "\n",
        "    def _get_json(self, url_post, callee, params=None):\n",
        "        # Implement a basic rate limit check\n",
        "        time_since_last_request = time.time() - self._last_requests[callee]\n",
        "        if time_since_last_request < self._rate_limit_seconds:\n",
        "            # Wait if we're hitting the rate limit too fast\n",
        "            sleep_time = self._rate_limit_seconds - time_since_last_request\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'application/json',\n",
        "            'Authorization': f'Bearer {COINCAP_API_KEY}'\n",
        "        }\n",
        "\n",
        "        r = requests.get(f\"{self._api_url}{url_post}\", headers=headers, params=params, timeout=60.00)\n",
        "        if r.status_code == 200:\n",
        "            self._last_requests[callee] = time.time()\n",
        "            json_response = r.json()\n",
        "            return json_response\n",
        "        logger.debug(f\"Response not OK. Status {r.status_code} - {r.reason}\")\n",
        "        print(f\"DEBUG: API call to {f'{self._api_url}{url_post}'} failed with status {r.status_code}: {r.reason}. Response text: {r.text}\")\n",
        "        return None\n",
        "\n",
        "    def get_asset(self, asset_id=\"bitcoin\"):\n",
        "        \"\"\"Fetches data for a specific cryptocurrency asset.\"\"\"\n",
        "        json_data = self._get_json(f\"/assets/{asset_id}\", self.get_asset)\n",
        "        return CoinCapResponse(json_data) if json_data else None\n",
        "\n",
        "    def get_asset_history(self, asset_id, interval, start=None, end=None):\n",
        "        \"\"\"Fetches historical data for a specific cryptocurrency asset.\n",
        "\n",
        "        Args:\n",
        "            asset_id (str): The ID of the asset (e.g., \"bitcoin\").\n",
        "            interval (str): The time interval for historical data (e.g., 'd1', 'h1', 'm1').\n",
        "            start (int, optional): Unix timestamp (milliseconds) for the start of the period.\n",
        "            end (int, optional): Unix timestamp (milliseconds) for the end of the period.\n",
        "\n",
        "        Returns:\n",
        "            dict: The JSON response containing historical data, or None if an error occurs.\n",
        "        \"\"\"\n",
        "        url_post = f\"/assets/{asset_id}/history\"\n",
        "        params = {'interval': interval}\n",
        "\n",
        "        if start is not None:\n",
        "            params['start'] = int(start)\n",
        "        if end is not None:\n",
        "            params['end'] = int(end)\n",
        "\n",
        "        json_data = self._get_json(url_post, self.get_asset_history, params=params)\n",
        "        return json_data\n",
        "\n",
        "# ==============================================================================\n",
        "# Data Parser Functions\n",
        "# ==============================================================================\n",
        "\n",
        "def _convertTimestamp(timestamp_seconds):\n",
        "    \"\"\"Converts a Unix timestamp (in seconds) to a formatted datetime string.\"\"\"\n",
        "    if timestamp_seconds is not None:\n",
        "        try:\n",
        "            return datetime.datetime.fromtimestamp(timestamp_seconds).strftime('%Y-%m-%d %H:%M:%S')\n",
        "        except Exception:\n",
        "            logger.error(f\"Error converting timestamp {timestamp_seconds}\", exc_info=True)\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "def _convert(data, dataType):\n",
        "    \"\"\"Converts data to the specified dataType, handling None and type conversion errors.\"\"\"\n",
        "    if data is None:\n",
        "        return None\n",
        "    if dataType == str:\n",
        "        # For CoinCap API, some string values might already be clean, but strip() is safe.\n",
        "        return str(data).strip()\n",
        "    try:\n",
        "        return dataType(data)\n",
        "    except Exception:\n",
        "        logger.error(f\"Error converting data '{data}' to {dataType.__name__}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "def _convertRow(coincap_response):\n",
        "    \"\"\"Converts a CoinCapResponse object into a dictionary suitable for BigQuery.\n",
        "    Assumes coincap_response contains a CoinCapAsset object under .data and a timestamp_seconds.\n",
        "    \"\"\"\n",
        "    if coincap_response is None or coincap_response.data is None:\n",
        "        return {}\n",
        "\n",
        "    asset_data = coincap_response.data\n",
        "    row = {\n",
        "        'id': _convert(asset_data.id, str),\n",
        "        'rank': _convert(asset_data.rank, int),\n",
        "        'symbol': _convert(asset_data.symbol, str),\n",
        "        'name': _convert(asset_data.name, str),\n",
        "        'supply': _convert(asset_data.supply, float),\n",
        "        'maxSupply': _convert(asset_data.maxSupply, float),\n",
        "        'marketCapUsd': _convert(asset_data.marketCapUsd, float),\n",
        "        'volumeUsd24Hr': _convert(asset_data.volumeUsd24Hr, float),\n",
        "        'priceUsd': _convert(asset_data.priceUsd, float),\n",
        "        'changePercent24Hr': _convert(asset_data.changePercent24Hr, float),\n",
        "        'vwap24Hr': _convert(asset_data.vwap24Hr, float),\n",
        "        'explorer': _convert(asset_data.explorer, str)\n",
        "    }\n",
        "\n",
        "    # Add the CoinCap API response timestamp\n",
        "    if coincap_response.timestamp_seconds is not None:\n",
        "        row['coincap_query_time_bq'] = _convertTimestamp(coincap_response.timestamp_seconds)\n",
        "\n",
        "    # Return only non-null values, as BQ handles missing fields\n",
        "    return {k: v for k, v in row.items() if v is not None}\n",
        "\n",
        "def _convertHistoricalRecord(historical_data_point, asset_id):\n",
        "    \"\"\"Converts a single historical data point into a dictionary suitable for BigQuery.\n",
        "\n",
        "    Args:\n",
        "        historical_data_point (dict): A dictionary representing a single historical entry from the CoinCap API response.\n",
        "                                      Expected keys: 'priceUsd', 'time', and potentially others like 'volume'.\n",
        "        asset_id (str): The ID of the asset (e.g., \"bitcoin\").\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the formatted historical record, or an empty dict if conversion fails.\n",
        "    \"\"\"\n",
        "    if not historical_data_point:\n",
        "        return {}\n",
        "\n",
        "    # Extract and convert priceUsd\n",
        "    price_usd = _convert(historical_data_point.get('priceUsd'), float)\n",
        "\n",
        "    # Extract and convert timestamp from milliseconds to formatted datetime string\n",
        "    timestamp_ms = historical_data_point.get('time')\n",
        "    coincap_query_time_bq = None\n",
        "    if timestamp_ms is not None:\n",
        "        coincap_query_time_bq = _convertTimestamp(timestamp_ms / 1000)\n",
        "\n",
        "    # Construct the record dictionary\n",
        "    record = {\n",
        "        'id': asset_id,\n",
        "        'priceUsd': price_usd,\n",
        "        'coincap_query_time_bq': coincap_query_time_bq\n",
        "    }\n",
        "\n",
        "    # Optionally add other fields if available in historical_data_point and relevant\n",
        "    # For instance, if 'volume' is present and we want to include it:\n",
        "    # record['volumeUsd24Hr'] = _convert(historical_data_point.get('volume'), float)\n",
        "\n",
        "    # Return only non-null values\n",
        "    return {k: v for k, v in record.items() if v is not None}\n",
        "\n",
        "# Re-initialize GCP clients (ensuring they are available in this scope)\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Redefine load_gcs_to_bigquery (if it uses any global variables that might be affected)\n",
        "# This function does not directly depend on the CoinCapApi class but is part of the pipeline context.\n",
        "def load_gcs_to_bigquery(gcs_uri, project_id, bq_dataset, bq_table, bq_schema, bq_client_instance):\n",
        "    \"\"\"Loads data from a GCS URI into a BigQuery table.\"\"\"\n",
        "    print(f\"\\nStep: Loading data from GCS into BigQuery...\")\n",
        "    print(f\"  > Source: {gcs_uri}\")\n",
        "    print(f\"  > Target: {bq_dataset}.{bq_table}\")\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
        "    job_config.schema = bq_schema\n",
        "    job_config.autodetect = False\n",
        "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
        "\n",
        "    load_job = bq_client_instance.load_table_from_uri(\n",
        "        gcs_uri,\n",
        "        f\"{project_id}.{bq_dataset}.{bq_table}\",\n",
        "        job_config=job_config\n",
        "    )\n",
        "\n",
        "    print(f\"  > Starting BQ Load Job: {load_job.job_id}\")\n",
        "    load_job.result()\n",
        "    print(f\"  > Job complete. Loaded {load_job.output_rows} rows.\")\n",
        "    print(\"âœ… GCS to BigQuery Load Finished Successfully.\")\n",
        "\n",
        "def run_full_pipeline_without_bq_load():\n",
        "    \"\"\"Executes the API -> GCS pipeline for CoinCap data and returns GCS URI.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # ======================================================\n",
        "        # 1. Download data from CoinCap API\n",
        "        # ======================================================\n",
        "        print(f\"Step 1: Fetching Bitcoin historical data from CoinCap API...\")\n",
        "        api = CoinCapApi()\n",
        "        queryTime = datetime.datetime.now().timestamp()\n",
        "\n",
        "        # Define asset_id for historical data fetch\n",
        "        asset_id_to_fetch = \"bitcoin\"\n",
        "\n",
        "        # Calculate start and end timestamps for the last 7 days\n",
        "        end_datetime = datetime.datetime.now()\n",
        "        end_timestamp_ms = int(end_datetime.timestamp() * 1000)\n",
        "        start_datetime = end_datetime - datetime.timedelta(days=7)\n",
        "        start_timestamp_ms = int(start_datetime.timestamp() * 1000)\n",
        "\n",
        "        # Fetch historical data\n",
        "        historical_data_response = api.get_asset_history(\n",
        "            asset_id=asset_id_to_fetch,\n",
        "            interval=\"d1\", # Daily interval\n",
        "            start=start_timestamp_ms,\n",
        "            end=end_timestamp_ms\n",
        "        )\n",
        "\n",
        "        records = []\n",
        "        if historical_data_response and 'data' in historical_data_response:\n",
        "            for data_point in historical_data_response['data']:\n",
        "                # Use the new _convertHistoricalRecord function\n",
        "                record = _convertHistoricalRecord(data_point, asset_id_to_fetch)\n",
        "                if record:\n",
        "                    records.append(record)\n",
        "\n",
        "        if not records:\n",
        "            print(\"No Bitcoin historical data found. Exiting.\")\n",
        "            return None # Return None if no records\n",
        "\n",
        "        print(f\"  > Fetched {len(records)} record(s).\")\n",
        "\n",
        "        # ======================================================\n",
        "        # 2. Save data to GCS Bucket\n",
        "        # ======================================================\n",
        "        local_filename = \"bitcoin_historical_data.jsonl\"\n",
        "        with open(local_filename, 'w') as f:\n",
        "            for record in records:\n",
        "                f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "        gcs_filename = f\"{GCS_FOLDER_PATH}/coincap_history_batch_{int(queryTime)}.jsonl\"\n",
        "\n",
        "        print(f\"\\nStep 2: Uploading data to GCS...\")\n",
        "        print(f\"  > Source: {local_filename}\")\n",
        "        print(f\"  > Destination: gs://{GCS_BUCKET_NAME}/{gcs_filename}\")\n",
        "\n",
        "        bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
        "        blob = bucket.blob(gcs_filename)\n",
        "        blob.upload_from_filename(local_filename)\n",
        "\n",
        "        gcs_uri = f\"gs://{GCS_BUCKET_NAME}/{gcs_filename}\"\n",
        "        print(\"  > Upload complete.\")\n",
        "        print(\"âœ… API to GCS Pipeline Finished Successfully.\")\n",
        "        return gcs_uri # Return the GCS URI\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ ERROR in pipeline: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"âœ… CoinCap API class and pipeline functions redefined with _convertHistoricalRecord and updated run_full_pipeline_without_bq_load.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… CoinCap API class and pipeline functions redefined with _convertHistoricalRecord and updated run_full_pipeline_without_bq_load.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "660ebe8f"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to update the `BQ_SCHEMA` to match the output of the `_convertHistoricalRecord` function. I have redefined the `BQ_SCHEMA` list to include only `id`, `priceUsd`, and `coincap_query_time_bq` with their appropriate data types and `NULLABLE` mode. The rest of the code in the cell remains the same to maintain self-containment and consistency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75cc82b9",
        "outputId": "976099ee-c3b5-4a44-d1ef-955c503adae9"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import datetime\n",
        "import time\n",
        "import requests\n",
        "from collections import defaultdict\n",
        "from google.cloud import storage, bigquery\n",
        "\n",
        "# --- !! CONFIGURE YOUR VARIABLES (re-defined for self-containment) !! ---\n",
        "PROJECT_ID = \"directed-bongo-471119-d1\"\n",
        "GCP_REGION = \"us-central1\"\n",
        "GCS_BUCKET_NAME = \"467_final_3_2\"\n",
        "GCS_FOLDER_PATH = \"data_1\"\n",
        "BQ_DATASET = \"467_final_3_2\"\n",
        "BQ_TABLE = \"data_1_t\"\n",
        "FLIGHT_RECORD_LIMIT = 500 # Not used in this specific CoinCap context, but kept for consistency if pipeline structure changes\n",
        "\n",
        "COINCAP_API_KEY = \"7aaeaf52f203c5f61c86c294210dada652fc5adc8d566875faa9eedebbbdf761\" # Re-define API key\n",
        "\n",
        "# Define BigQuery Schema based on _convertHistoricalRecord output\n",
        "BQ_SCHEMA = [\n",
        "    bigquery.SchemaField(\"id\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"priceUsd\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"coincap_query_time_bq\", \"TIMESTAMP\", mode=\"NULLABLE\")\n",
        "]\n",
        "\n",
        "# ==============================================================================\n",
        "# CoinCap API Library Code (Modified to use API Key and add get_asset_history)\n",
        "# ==============================================================================\n",
        "\n",
        "logger = logging.getLogger('coincap_api')\n",
        "logger.addHandler(logging.NullHandler())\n",
        "\n",
        "class CoinCapAsset(object):\n",
        "    \"\"\"Represents a single cryptocurrency asset from the CoinCap API data.\"\"\"\n",
        "    keys = [\"id\", \"rank\", \"symbol\", \"name\", \"supply\", \"maxSupply\",\n",
        "            \"marketCapUsd\", \"volumeUsd24Hr\", \"priceUsd\", \"changePercent24Hr\",\n",
        "            \"vwap24Hr\", \"explorer\"]\n",
        "\n",
        "    def __init__(self, data_dict):\n",
        "        # Initialize attributes from the data_dict\n",
        "        for key in CoinCapAsset.keys:\n",
        "            # Use .get() to safely retrieve values, in case a key is missing\n",
        "            setattr(self, key, data_dict.get(key))\n",
        "\n",
        "class CoinCapResponse(object):\n",
        "    \"\"\"Represents the overall response from the CoinCap API for an asset.\"\"\"\n",
        "    def __init__(self, j):\n",
        "        self.__dict__ = j\n",
        "        # If 'data' exists and is not None, parse it into a CoinCapAsset object\n",
        "        if 'data' in self.__dict__ and self.data is not None:\n",
        "            self.data = CoinCapAsset(self.data)\n",
        "        else:\n",
        "            self.data = None\n",
        "        # Convert timestamp from milliseconds to seconds for general use\n",
        "        if 'timestamp' in self.__dict__ and self.timestamp is not None:\n",
        "            self.timestamp_seconds = int(self.timestamp / 1000)\n",
        "        else:\n",
        "            self.timestamp_seconds = None\n",
        "\n",
        "\n",
        "class CoinCapApi(object):\n",
        "    def __init__(self):\n",
        "        self._api_url = \"https://rest.coincap.io/v3\"\n",
        "        self._last_requests = defaultdict(lambda: 0)\n",
        "        # CoinCap has a rate limit of 2000 requests per minute, so ~0.03 seconds per request\n",
        "        self._rate_limit_seconds = 60 / 2000 # 0.03 seconds per request\n",
        "\n",
        "    def _get_json(self, url_post, callee, params=None):\n",
        "        # Implement a basic rate limit check\n",
        "        time_since_last_request = time.time() - self._last_requests[callee]\n",
        "        if time_since_last_request < self._rate_limit_seconds:\n",
        "            # Wait if we're hitting the rate limit too fast\n",
        "            sleep_time = self._rate_limit_seconds - time_since_last_request\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'application/json',\n",
        "            'Authorization': f'Bearer {COINCAP_API_KEY}'\n",
        "        }\n",
        "\n",
        "        r = requests.get(f\"{self._api_url}{url_post}\", headers=headers, params=params, timeout=60.00)\n",
        "        if r.status_code == 200:\n",
        "            self._last_requests[callee] = time.time()\n",
        "            json_response = r.json()\n",
        "            return json_response\n",
        "        logger.debug(f\"Response not OK. Status {r.status_code} - {r.reason}\")\n",
        "        print(f\"DEBUG: API call to {f'{self._api_url}{url_post}'} failed with status {r.status_code}: {r.reason}. Response text: {r.text}\")\n",
        "        return None\n",
        "\n",
        "    def get_asset(self, asset_id=\"bitcoin\"):\n",
        "        \"\"\"Fetches data for a specific cryptocurrency asset.\"\"\"\n",
        "        json_data = self._get_json(f\"/assets/{asset_id}\", self.get_asset)\n",
        "        return CoinCapResponse(json_data) if json_data else None\n",
        "\n",
        "    def get_asset_history(self, asset_id, interval, start=None, end=None):\n",
        "        \"\"\"Fetches historical data for a specific cryptocurrency asset.\n",
        "\n",
        "        Args:\n",
        "            asset_id (str): The ID of the asset (e.g., \"bitcoin\").\n",
        "            interval (str): The time interval for historical data (e.g., 'd1', 'h1', 'm1').\n",
        "            start (int, optional): Unix timestamp (milliseconds) for the start of the period.\n",
        "            end (int, optional): Unix timestamp (milliseconds) for the end of the period.\n",
        "\n",
        "        Returns:\n",
        "            dict: The JSON response containing historical data, or None if an error occurs.\n",
        "        \"\"\"\n",
        "        url_post = f\"/assets/{asset_id}/history\"\n",
        "        params = {'interval': interval}\n",
        "\n",
        "        if start is not None:\n",
        "            params['start'] = int(start)\n",
        "        if end is not None:\n",
        "            params['end'] = int(end)\n",
        "\n",
        "        json_data = self._get_json(url_post, self.get_asset_history, params=params)\n",
        "        return json_data\n",
        "\n",
        "# ==============================================================================\n",
        "# Data Parser Functions\n",
        "# ==============================================================================\n",
        "\n",
        "def _convertTimestamp(timestamp_seconds):\n",
        "    \"\"\"Converts a Unix timestamp (in seconds) to a formatted datetime string.\"\"\"\n",
        "    if timestamp_seconds is not None:\n",
        "        try:\n",
        "            return datetime.datetime.fromtimestamp(timestamp_seconds).strftime('%Y-%m-%d %H:%M:%S')\n",
        "        except Exception:\n",
        "            logger.error(f\"Error converting timestamp {timestamp_seconds}\", exc_info=True)\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "def _convert(data, dataType):\n",
        "    \"\"\"Converts data to the specified dataType, handling None and type conversion errors.\"\"\"\n",
        "    if data is None:\n",
        "        return None\n",
        "    if dataType == str:\n",
        "        # For CoinCap API, some string values might already be clean, but strip() is safe.\n",
        "        return str(data).strip()\n",
        "    try:\n",
        "        return dataType(data)\n",
        "    except Exception:\n",
        "        logger.error(f\"Error converting data '{data}' to {dataType.__name__}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "def _convertRow(coincap_response):\n",
        "    \"\"\"Converts a CoinCapResponse object into a dictionary suitable for BigQuery.\n",
        "    Assumes coincap_response contains a CoinCapAsset object under .data and a timestamp_seconds.\n",
        "    \"\"\"\n",
        "    if coincap_response is None or coincap_response.data is None:\n",
        "        return {}\n",
        "\n",
        "    asset_data = coincap_response.data\n",
        "    row = {\n",
        "        'id': _convert(asset_data.id, str),\n",
        "        'rank': _convert(asset_data.rank, int),\n",
        "        'symbol': _convert(asset_data.symbol, str),\n",
        "        'name': _convert(asset_data.name, str),\n",
        "        'supply': _convert(asset_data.supply, float),\n",
        "        'maxSupply': _convert(asset_data.maxSupply, float),\n",
        "        'marketCapUsd': _convert(asset_data.marketCapUsd, float),\n",
        "        'volumeUsd24Hr': _convert(asset_data.volumeUsd24Hr, float),\n",
        "        'priceUsd': _convert(asset_data.priceUsd, float),\n",
        "        'changePercent24Hr': _convert(asset_data.changePercent24Hr, float),\n",
        "        'vwap24Hr': _convert(asset_data.vwap24Hr, float),\n",
        "        'explorer': _convert(asset_data.explorer, str)\n",
        "    }\n",
        "\n",
        "    # Add the CoinCap API response timestamp\n",
        "    if coincap_response.timestamp_seconds is not None:\n",
        "        row['coincap_query_time_bq'] = _convertTimestamp(coincap_response.timestamp_seconds)\n",
        "\n",
        "    # Return only non-null values, as BQ handles missing fields\n",
        "    return {k: v for k, v in row.items() if v is not None}\n",
        "\n",
        "def _convertHistoricalRecord(historical_data_point, asset_id):\n",
        "    \"\"\"Converts a single historical data point into a dictionary suitable for BigQuery.\n",
        "\n",
        "    Args:\n",
        "        historical_data_point (dict): A dictionary representing a single historical entry from the CoinCap API response.\n",
        "                                      Expected keys: 'priceUsd', 'time', and potentially others like 'volume'.\n",
        "        asset_id (str): The ID of the asset (e.g., \"bitcoin\").\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the formatted historical record, or an empty dict if conversion fails.\n",
        "    \"\"\"\n",
        "    if not historical_data_point:\n",
        "        return {}\n",
        "\n",
        "    # Extract and convert priceUsd\n",
        "    price_usd = _convert(historical_data_point.get('priceUsd'), float)\n",
        "\n",
        "    # Extract and convert timestamp from milliseconds to formatted datetime string\n",
        "    timestamp_ms = historical_data_point.get('time')\n",
        "    coincap_query_time_bq = None\n",
        "    if timestamp_ms is not None:\n",
        "        coincap_query_time_bq = _convertTimestamp(timestamp_ms / 1000)\n",
        "\n",
        "    # Construct the record dictionary\n",
        "    record = {\n",
        "        'id': asset_id,\n",
        "        'priceUsd': price_usd,\n",
        "        'coincap_query_time_bq': coincap_query_time_bq\n",
        "    }\n",
        "\n",
        "    # Optionally add other fields if available in historical_data_point and relevant\n",
        "    # For instance, if 'volume' is present and we want to include it:\n",
        "    # record['volumeUsd24Hr'] = _convert(historical_data_point.get('volume'), float)\n",
        "\n",
        "    # Return only non-null values\n",
        "    return {k: v for k, v in record.items() if v is not None}\n",
        "\n",
        "# Re-initialize GCP clients (ensuring they are available in this scope)\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Redefine load_gcs_to_bigquery (if it uses any global variables that might be affected)\n",
        "# This function does not directly depend on the CoinCapApi class but is part of the pipeline context.\n",
        "def load_gcs_to_bigquery(gcs_uri, project_id, bq_dataset, bq_table, bq_schema, bq_client_instance):\n",
        "    \"\"\"Loads data from a GCS URI into a BigQuery table.\"\"\"\n",
        "    print(f\"\\nStep: Loading data from GCS into BigQuery...\")\n",
        "    print(f\"  > Source: {gcs_uri}\")\n",
        "    print(f\"  > Target: {bq_dataset}.{bq_table}\")\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
        "    job_config.schema = bq_schema\n",
        "    job_config.autodetect = False\n",
        "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
        "\n",
        "    load_job = bq_client_instance.load_table_from_uri(\n",
        "        gcs_uri,\n",
        "        f\"{project_id}.{bq_dataset}.{bq_table}\",\n",
        "        job_config=job_config\n",
        "    )\n",
        "\n",
        "    print(f\"  > Starting BQ Load Job: {load_job.job_id}\")\n",
        "    load_job.result()\n",
        "    print(f\"  > Job complete. Loaded {load_job.output_rows} rows.\")\n",
        "    print(\"âœ… GCS to BigQuery Load Finished Successfully.\")\n",
        "\n",
        "def run_full_pipeline_without_bq_load():\n",
        "    \"\"\"Executes the API -> GCS pipeline for CoinCap data and returns GCS URI.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # ======================================================\n",
        "        # 1. Download data from CoinCap API\n",
        "        # ======================================================\n",
        "        print(f\"Step 1: Fetching Bitcoin historical data from CoinCap API...\")\n",
        "        api = CoinCapApi()\n",
        "        queryTime = datetime.datetime.now().timestamp()\n",
        "\n",
        "        # Define asset_id for historical data fetch\n",
        "        asset_id_to_fetch = \"bitcoin\"\n",
        "\n",
        "        # Calculate start and end timestamps for the last 7 days\n",
        "        end_datetime = datetime.datetime.now()\n",
        "        end_timestamp_ms = int(end_datetime.timestamp() * 1000)\n",
        "        start_datetime = end_datetime - datetime.timedelta(days=7)\n",
        "        start_timestamp_ms = int(start_datetime.timestamp() * 1000)\n",
        "\n",
        "        # Fetch historical data\n",
        "        historical_data_response = api.get_asset_history(\n",
        "            asset_id=asset_id_to_fetch,\n",
        "            interval=\"d1\", # Daily interval\n",
        "            start=start_timestamp_ms,\n",
        "            end=end_timestamp_ms\n",
        "        )\n",
        "\n",
        "        records = []\n",
        "        if historical_data_response and 'data' in historical_data_response:\n",
        "            for data_point in historical_data_response['data']:\n",
        "                # Use the new _convertHistoricalRecord function\n",
        "                record = _convertHistoricalRecord(data_point, asset_id_to_fetch)\n",
        "                if record:\n",
        "                    records.append(record)\n",
        "\n",
        "        if not records:\n",
        "            print(\"No Bitcoin historical data found. Exiting.\")\n",
        "            return None # Return None if no records\n",
        "\n",
        "        print(f\"  > Fetched {len(records)} record(s).\")\n",
        "\n",
        "        # ======================================================\n",
        "        # 2. Save data to GCS Bucket\n",
        "        # ======================================================\n",
        "        local_filename = \"bitcoin_historical_data.jsonl\"\n",
        "        with open(local_filename, 'w') as f:\n",
        "            for record in records:\n",
        "                f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "        gcs_filename = f\"{GCS_FOLDER_PATH}/coincap_history_batch_{int(queryTime)}.jsonl\"\n",
        "\n",
        "        print(f\"\\nStep 2: Uploading data to GCS...\")\n",
        "        print(f\"  > Source: {local_filename}\")\n",
        "        print(f\"  > Destination: gs://{GCS_BUCKET_NAME}/{gcs_filename}\")\n",
        "\n",
        "        bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
        "        blob = bucket.blob(gcs_filename)\n",
        "        blob.upload_from_filename(local_filename)\n",
        "\n",
        "        gcs_uri = f\"gs://{GCS_BUCKET_NAME}/{gcs_filename}\"\n",
        "        print(\"  > Upload complete.\")\n",
        "        print(\"âœ… API to GCS Pipeline Finished Successfully.\")\n",
        "        return gcs_uri # Return the GCS URI\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ ERROR in pipeline: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"âœ… CoinCap API class and pipeline functions redefined with _convertHistoricalRecord and updated run_full_pipeline_without_bq_load.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… CoinCap API class and pipeline functions redefined with _convertHistoricalRecord and updated run_full_pipeline_without_bq_load.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "787057bb"
      },
      "source": [
        "## Execute Updated Pipeline\n",
        "\n",
        "### Subtask:\n",
        "Run the complete data pipeline (API -> GCS -> BigQuery) with the modified CoinCapApi and run_full_pipeline_without_bq_load functions to fetch, process, and load the historical Bitcoin data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "926a8e27"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires executing the complete data pipeline, including checking for and creating the BigQuery table if it doesn't exist, fetching historical data, uploading to GCS, and loading into BigQuery. I will redefine the relevant code block to incorporate the table creation/check logic before proceeding with the data pipeline execution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fcb1c2f",
        "outputId": "a90d1de0-4672-4b52-a336-6aa63f3daae1"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import datetime\n",
        "import time\n",
        "import requests\n",
        "from collections import defaultdict\n",
        "from google.cloud import storage, bigquery\n",
        "from google.cloud.exceptions import NotFound # Import NotFound for specific error handling\n",
        "\n",
        "# --- !! CONFIGURE YOUR VARIABLES (re-defined for self-containment) !! ---\n",
        "PROJECT_ID = \"directed-bongo-471119-d1\"\n",
        "GCP_REGION = \"us-central1\"\n",
        "GCS_BUCKET_NAME = \"467_final_3_2\"\n",
        "GCS_FOLDER_PATH = \"data_1\"\n",
        "BQ_DATASET = \"467_final_3_2\"\n",
        "BQ_TABLE = \"data_1_t\"\n",
        "FLIGHT_RECORD_LIMIT = 500 # Not used in this specific CoinCap context, but kept for consistency if pipeline structure changes\n",
        "\n",
        "COINCAP_API_KEY = \"7aaeaf52f203c5f61c86c294210dada652fc5adc8d566875faa9eedebbbdf761\" # Re-define API key\n",
        "\n",
        "# Define BigQuery Schema based on _convertHistoricalRecord output\n",
        "BQ_SCHEMA = [\n",
        "    bigquery.SchemaField(\"id\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"priceUsd\", \"FLOAT\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"coincap_query_time_bq\", \"TIMESTAMP\", mode=\"NULLABLE\")\n",
        "]\n",
        "\n",
        "# ==============================================================================\n",
        "# CoinCap API Library Code (Modified to use API Key and add get_asset_history)\n",
        "# ==============================================================================\n",
        "\n",
        "logger = logging.getLogger('coincap_api')\n",
        "logger.addHandler(logging.NullHandler())\n",
        "\n",
        "class CoinCapAsset(object):\n",
        "    \"\"\"Represents a single cryptocurrency asset from the CoinCap API data.\"\"\"\n",
        "    keys = [\"id\", \"rank\", \"symbol\", \"name\", \"supply\", \"maxSupply\",\n",
        "            \"marketCapUsd\", \"volumeUsd24Hr\", \"priceUsd\", \"changePercent24Hr\",\n",
        "            \"vwap24Hr\", \"explorer\"]\n",
        "\n",
        "    def __init__(self, data_dict):\n",
        "        # Initialize attributes from the data_dict\n",
        "        for key in CoinCapAsset.keys:\n",
        "            # Use .get() to safely retrieve values, in case a key is missing\n",
        "            setattr(self, key, data_dict.get(key))\n",
        "\n",
        "class CoinCapResponse(object):\n",
        "    \"\"\"Represents the overall response from the CoinCap API for an asset.\"\"\"\n",
        "    def __init__(self, j):\n",
        "        self.__dict__ = j\n",
        "        # If 'data' exists and is not None, parse it into a CoinCapAsset object\n",
        "        if 'data' in self.__dict__ and self.data is not None:\n",
        "            self.data = CoinCapAsset(self.data)\n",
        "        else:\n",
        "            self.data = None\n",
        "        # Convert timestamp from milliseconds to seconds for general use\n",
        "        if 'timestamp' in self.__dict__ and self.timestamp is not None:\n",
        "            self.timestamp_seconds = int(self.timestamp / 1000)\n",
        "        else:\n",
        "            self.timestamp_seconds = None\n",
        "\n",
        "\n",
        "class CoinCapApi(object):\n",
        "    def __init__(self):\n",
        "        self._api_url = \"https://rest.coincap.io/v3\"\n",
        "        self._last_requests = defaultdict(lambda: 0)\n",
        "        # CoinCap has a rate limit of 2000 requests per minute, so ~0.03 seconds per request\n",
        "        self._rate_limit_seconds = 60 / 2000 # 0.03 seconds per request\n",
        "\n",
        "    def _get_json(self, url_post, callee, params=None):\n",
        "        # Implement a basic rate limit check\n",
        "        time_since_last_request = time.time() - self._last_requests[callee]\n",
        "        if time_since_last_request < self._rate_limit_seconds:\n",
        "            # Wait if we're hitting the rate limit too fast\n",
        "            sleep_time = self._rate_limit_seconds - time_since_last_request\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'application/json',\n",
        "            'Authorization': f'Bearer {COINCAP_API_KEY}'\n",
        "        }\n",
        "\n",
        "        r = requests.get(f\"{self._api_url}{url_post}\", headers=headers, params=params, timeout=60.00)\n",
        "        if r.status_code == 200:\n",
        "            self._last_requests[callee] = time.time()\n",
        "            json_response = r.json()\n",
        "            return json_response\n",
        "        logger.debug(f\"Response not OK. Status {r.status_code} - {r.reason}\")\n",
        "        print(f\"DEBUG: API call to {f'{self._api_url}{url_post}'} failed with status {r.status_code}: {r.reason}. Response text: {r.text}\")\n",
        "        return None\n",
        "\n",
        "    def get_asset(self, asset_id=\"bitcoin\"):\n",
        "        \"\"\"Fetches data for a specific cryptocurrency asset.\"\"\"\n",
        "        json_data = self._get_json(f\"/assets/{asset_id}\", self.get_asset)\n",
        "        return CoinCapResponse(json_data) if json_data else None\n",
        "\n",
        "    def get_asset_history(self, asset_id, interval, start=None, end=None):\n",
        "        \"\"\"Fetches historical data for a specific cryptocurrency asset.\n",
        "\n",
        "        Args:\n",
        "            asset_id (str): The ID of the asset (e.g., \"bitcoin\").\n",
        "            interval (str): The time interval for historical data (e.g., 'd1', 'h1', 'm1').\n",
        "            start (int, optional): Unix timestamp (milliseconds) for the start of the period.\n",
        "            end (int, optional): Unix timestamp (milliseconds) for the end of the period.\n",
        "\n",
        "        Returns:\n",
        "            dict: The JSON response containing historical data, or None if an error occurs.\n",
        "        \"\"\"\n",
        "        url_post = f\"/assets/{asset_id}/history\"\n",
        "        params = {'interval': interval}\n",
        "\n",
        "        if start is not None:\n",
        "            params['start'] = int(start)\n",
        "        if end is not None:\n",
        "            params['end'] = int(end)\n",
        "\n",
        "        json_data = self._get_json(url_post, self.get_asset_history, params=params)\n",
        "        return json_data\n",
        "\n",
        "# ==============================================================================\n",
        "# Data Parser Functions\n",
        "# ==============================================================================\n",
        "\n",
        "def _convertTimestamp(timestamp_seconds):\n",
        "    \"\"\"Converts a Unix timestamp (in seconds) to a formatted datetime string.\"\"\"\n",
        "    if timestamp_seconds is not None:\n",
        "        try:\n",
        "            return datetime.datetime.fromtimestamp(timestamp_seconds).strftime('%Y-%m-%d %H:%M:%S')\n",
        "        except Exception:\n",
        "            logger.error(f\"Error converting timestamp {timestamp_seconds}\", exc_info=True)\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "def _convert(data, dataType):\n",
        "    \"\"\"Converts data to the specified dataType, handling None and type conversion errors.\"\"\"\n",
        "    if data is None:\n",
        "        return None\n",
        "    if dataType == str:\n",
        "        # For CoinCap API, some string values might already be clean, but strip() is safe.\n",
        "        return str(data).strip()\n",
        "    try:\n",
        "        return dataType(data)\n",
        "    except Exception:\n",
        "        logger.error(f\"Error converting data '{data}' to {dataType.__name__}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "def _convertRow(coincap_response):\n",
        "    \"\"\"Converts a CoinCapResponse object into a dictionary suitable for BigQuery.\n",
        "    Assumes coincap_response contains a CoinCapAsset object under .data and a timestamp_seconds.\n",
        "    \"\"\"\n",
        "    if coincap_response is None or coincap_response.data is None:\n",
        "        return {}\n",
        "\n",
        "    asset_data = coincap_response.data\n",
        "    row = {\n",
        "        'id': _convert(asset_data.id, str),\n",
        "        'rank': _convert(asset_data.rank, int),\n",
        "        'symbol': _convert(asset_data.symbol, str),\n",
        "        'name': _convert(asset_data.name, str),\n",
        "        'supply': _convert(asset_data.supply, float),\n",
        "        'maxSupply': _convert(asset_data.maxSupply, float),\n",
        "        'marketCapUsd': _convert(asset_data.marketCapUsd, float),\n",
        "        'volumeUsd24Hr': _convert(asset_data.volumeUsd24Hr, float),\n",
        "        'priceUsd': _convert(asset_data.priceUsd, float),\n",
        "        'changePercent24Hr': _convert(asset_data.changePercent24Hr, float),\n",
        "        'vwap24Hr': _convert(asset_data.vwap24Hr, float),\n",
        "        'explorer': _convert(asset_data.explorer, str)\n",
        "    }\n",
        "\n",
        "    # Add the CoinCap API response timestamp\n",
        "    if coincap_response.timestamp_seconds is not None:\n",
        "        row['coincap_query_time_bq'] = _convertTimestamp(coincap_response.timestamp_seconds)\n",
        "\n",
        "    # Return only non-null values, as BQ handles missing fields\n",
        "    return {k: v for k, v in row.items() if v is not None}\n",
        "\n",
        "def _convertHistoricalRecord(historical_data_point, asset_id):\n",
        "    \"\"\"Converts a single historical data point into a dictionary suitable for BigQuery.\n",
        "\n",
        "    Args:\n",
        "        historical_data_point (dict): A dictionary representing a single historical entry from the CoinCap API response.\n",
        "                                      Expected keys: 'priceUsd', 'time', and potentially others like 'volume'.\n",
        "        asset_id (str): The ID of the asset (e.g., \"bitcoin\").\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the formatted historical record, or an empty dict if conversion fails.\n",
        "    \"\"\"\n",
        "    if not historical_data_point:\n",
        "        return {}\n",
        "\n",
        "    # Extract and convert priceUsd\n",
        "    price_usd = _convert(historical_data_point.get('priceUsd'), float)\n",
        "\n",
        "    # Extract and convert timestamp from milliseconds to formatted datetime string\n",
        "    timestamp_ms = historical_data_point.get('time')\n",
        "    coincap_query_time_bq = None\n",
        "    if timestamp_ms is not None:\n",
        "        coincap_query_time_bq = _convertTimestamp(timestamp_ms / 1000)\n",
        "\n",
        "    # Construct the record dictionary\n",
        "    record = {\n",
        "        'id': asset_id,\n",
        "        'priceUsd': price_usd,\n",
        "        'coincap_query_time_bq': coincap_query_time_bq\n",
        "    }\n",
        "\n",
        "    # Optionally add other fields if available in historical_data_point and relevant\n",
        "    # For instance, if 'volume' is present and we want to include it:\n",
        "    # record['volumeUsd24Hr'] = _convert(historical_data_point.get('volume'), float)\n",
        "\n",
        "    # Return only non-null values\n",
        "    return {k: v for k, v in record.items() if v is not None}\n",
        "\n",
        "# Re-initialize GCP clients (ensuring they are available in this scope)\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Redefine load_gcs_to_bigquery (if it uses any global variables that might be affected)\n",
        "# This function does not directly depend on the CoinCapApi class but is part of the pipeline context.\n",
        "def load_gcs_to_bigquery(gcs_uri, project_id, bq_dataset, bq_table, bq_schema, bq_client_instance):\n",
        "    \"\"\"Loads data from a GCS URI into a BigQuery table.\"\"\"\n",
        "    print(f\"\\nStep: Loading data from GCS into BigQuery...\")\n",
        "    print(f\"  > Source: {gcs_uri}\")\n",
        "    print(f\"  > Target: {bq_dataset}.{bq_table}\")\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
        "    job_config.schema = bq_schema\n",
        "    job_config.autodetect = False\n",
        "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
        "\n",
        "    load_job = bq_client_instance.load_table_from_uri(\n",
        "        gcs_uri,\n",
        "        f\"{project_id}.{bq_dataset}.{bq_table}\",\n",
        "        job_config=job_config\n",
        "    )\n",
        "\n",
        "    print(f\"  > Starting BQ Load Job: {load_job.job_id}\")\n",
        "    load_job.result()\n",
        "    print(f\"  > Job complete. Loaded {load_job.output_rows} rows.\")\n",
        "    print(\"âœ… GCS to BigQuery Load Finished Successfully.\")\n",
        "\n",
        "def run_full_pipeline_without_bq_load():\n",
        "    \"\"\"Executes the API -> GCS pipeline for CoinCap data and returns GCS URI.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # ======================================================\n",
        "        # 1. Download data from CoinCap API\n",
        "        # ======================================================\n",
        "        print(f\"Step 1: Fetching Bitcoin historical data from CoinCap API...\")\n",
        "        api = CoinCapApi()\n",
        "        queryTime = datetime.datetime.now().timestamp()\n",
        "\n",
        "        # Define asset_id for historical data fetch\n",
        "        asset_id_to_fetch = \"bitcoin\"\n",
        "\n",
        "        # Calculate start and end timestamps for the last 7 days\n",
        "        end_datetime = datetime.datetime.now()\n",
        "        end_timestamp_ms = int(end_datetime.timestamp() * 1000)\n",
        "        start_datetime = end_datetime - datetime.timedelta(days=365)\n",
        "        start_timestamp_ms = int(start_datetime.timestamp() * 1000)\n",
        "\n",
        "        # Fetch historical data\n",
        "        historical_data_response = api.get_asset_history(\n",
        "            asset_id=asset_id_to_fetch,\n",
        "            interval=\"d1\", # Daily interval\n",
        "            start=start_timestamp_ms,\n",
        "            end=end_timestamp_ms\n",
        "        )\n",
        "\n",
        "        records = []\n",
        "        if historical_data_response and 'data' in historical_data_response:\n",
        "            for data_point in historical_data_response['data']:\n",
        "                # Use the new _convertHistoricalRecord function\n",
        "                record = _convertHistoricalRecord(data_point, asset_id_to_fetch)\n",
        "                if record:\n",
        "                    records.append(record)\n",
        "\n",
        "        if not records:\n",
        "            print(\"No Bitcoin historical data found. Exiting.\")\n",
        "            return None # Return None if no records\n",
        "\n",
        "        print(f\"  > Fetched {len(records)} record(s).\")\n",
        "\n",
        "        # ======================================================\n",
        "        # 2. Save data to GCS Bucket\n",
        "        # ======================================================\n",
        "        local_filename = \"bitcoin_historical_data.jsonl\"\n",
        "        with open(local_filename, 'w') as f:\n",
        "            for record in records:\n",
        "                f.write(json.dumps(record) + '\\n')\n",
        "\n",
        "        gcs_filename = f\"{GCS_FOLDER_PATH}/coincap_history_batch_{int(queryTime)}.jsonl\"\n",
        "\n",
        "        print(f\"\\nStep 2: Uploading data to GCS...\")\n",
        "        print(f\"  > Source: {local_filename}\")\n",
        "        print(f\"  > Destination: gs://{GCS_BUCKET_NAME}/{gcs_filename}\")\n",
        "\n",
        "        bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
        "        blob = bucket.blob(gcs_filename)\n",
        "        blob.upload_from_filename(local_filename)\n",
        "\n",
        "        gcs_uri = f\"gs://{GCS_BUCKET_NAME}/{gcs_filename}\"\n",
        "        print(\"  > Upload complete.\")\n",
        "        print(\"âœ… API to GCS Pipeline Finished Successfully.\")\n",
        "        return gcs_uri # Return the GCS URI\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ ERROR in pipeline: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "print(\"--- Running Full Data Pipeline (API -> GCS -> BigQuery) ---\")\n",
        "\n",
        "# 1. Ensure BigQuery table exists\n",
        "target_table_id = f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\"\n",
        "\n",
        "try:\n",
        "    bq_client.get_table(target_table_id)  # API request\n",
        "    print(f\"âœ… BigQuery table '{target_table_id}' already exists.\")\n",
        "except NotFound:\n",
        "    print(f\"BigQuery table '{target_table_id}' not found. Creating table...\")\n",
        "    table = bigquery.Table(target_table_id, schema=BQ_SCHEMA)\n",
        "    table = bq_client.create_table(table)  # API request\n",
        "    print(f\"âœ… Created table {table.project}.{table.dataset_id}.{table.table_id}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error checking or creating BigQuery table: {e}\")\n",
        "    exit() # Exit if table cannot be ensured\n",
        "\n",
        "# 2. Execute API -> GCS pipeline\n",
        "gcs_uri_for_bq_load = run_full_pipeline_without_bq_load()\n",
        "\n",
        "if gcs_uri_for_bq_load:\n",
        "    # 3. Load data from GCS to BigQuery\n",
        "    load_gcs_to_bigquery(\n",
        "        gcs_uri_for_bq_load,\n",
        "        PROJECT_ID,\n",
        "        BQ_DATASET,\n",
        "        BQ_TABLE,\n",
        "        BQ_SCHEMA,\n",
        "        bq_client\n",
        "    )\n",
        "    print(\"âœ… Pipeline Finished Successfully.\")\n",
        "else:\n",
        "    print(\"âŒ Pipeline aborted: No data fetched or GCS upload failed.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Full Data Pipeline (API -> GCS -> BigQuery) ---\n",
            "âœ… BigQuery table 'directed-bongo-471119-d1.467_final_3_2.data_1_t' already exists.\n",
            "Step 1: Fetching Bitcoin historical data from CoinCap API...\n",
            "  > Fetched 366 record(s).\n",
            "\n",
            "Step 2: Uploading data to GCS...\n",
            "  > Source: bitcoin_historical_data.jsonl\n",
            "  > Destination: gs://467_final_3_2/data_1/coincap_history_batch_1765634428.jsonl\n",
            "  > Upload complete.\n",
            "âœ… API to GCS Pipeline Finished Successfully.\n",
            "\n",
            "Step: Loading data from GCS into BigQuery...\n",
            "  > Source: gs://467_final_3_2/data_1/coincap_history_batch_1765634428.jsonl\n",
            "  > Target: 467_final_3_2.data_1_t\n",
            "  > Starting BQ Load Job: 895c6232-51f6-482e-96c2-76dcccb4dcd6\n",
            "  > Job complete. Loaded 366 rows.\n",
            "âœ… GCS to BigQuery Load Finished Successfully.\n",
            "âœ… Pipeline Finished Successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize"
      ],
      "metadata": {
        "id": "fGscABcWkwfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize BigQuery client\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Construct the full table ID\n",
        "table_id = f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\"\n",
        "\n",
        "# SQL query to fetch Bitcoin price data\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "    coincap_query_time_bq AS Timestamp,\n",
        "    priceUsd AS BitcoinPrice\n",
        "FROM\n",
        "    `{table_id}`\n",
        "WHERE\n",
        "    id = 'bitcoin'\n",
        "ORDER BY\n",
        "    Timestamp ASC\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and load into a Pandas DataFrame\n",
        "try:\n",
        "    df_bitcoin_price = client.query(query).to_dataframe()\n",
        "    print(f\"Fetched {len(df_bitcoin_price)} Bitcoin price records.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error fetching data from BigQuery: {e}\")\n",
        "    df_bitcoin_price = pd.DataFrame()\n",
        "\n",
        "# Generate interactive Plotly chart if data is available\n",
        "if not df_bitcoin_price.empty:\n",
        "    fig = px.line(\n",
        "        df_bitcoin_price,\n",
        "        x='Timestamp',\n",
        "        y='BitcoinPrice',\n",
        "        title='Bitcoin Price Over 2025',\n",
        "        labels={'Timestamp': 'Date', 'BitcoinPrice': 'Price (USD)'}\n",
        "    )\n",
        "    fig.update_layout(\n",
        "        hovermode=\"x unified\",\n",
        "        xaxis_title=\"Date\",\n",
        "        yaxis_title=\"Price (USD)\"\n",
        "    )\n",
        "    fig.show()\n",
        "else:\n",
        "    print(\"No data available to plot.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "rVhEI4t8ky93",
        "outputId": "3275dab6-2184-42e0-ee60-a6f5eab73fc2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 366 Bitcoin price records.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"b577368f-1b29-4fcb-98c8-24567bcbe0ba\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b577368f-1b29-4fcb-98c8-24567bcbe0ba\")) {                    Plotly.newPlot(                        \"b577368f-1b29-4fcb-98c8-24567bcbe0ba\",                        [{\"hovertemplate\":\"Date=%{x}\\u003cbr\\u003ePrice (USD)=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[\"2024-12-13T15:00:00+00:00\",\"2024-12-14T00:00:00+00:00\",\"2024-12-15T00:00:00+00:00\",\"2024-12-16T00:00:00+00:00\",\"2024-12-17T00:00:00+00:00\",\"2024-12-18T00:00:00+00:00\",\"2024-12-19T00:00:00+00:00\",\"2024-12-20T00:00:00+00:00\",\"2024-12-21T00:00:00+00:00\",\"2024-12-22T00:00:00+00:00\",\"2024-12-23T00:00:00+00:00\",\"2024-12-24T00:00:00+00:00\",\"2024-12-25T00:00:00+00:00\",\"2024-12-26T00:00:00+00:00\",\"2024-12-27T00:00:00+00:00\",\"2024-12-28T00:00:00+00:00\",\"2024-12-29T00:00:00+00:00\",\"2024-12-30T00:00:00+00:00\",\"2024-12-31T00:00:00+00:00\",\"2025-01-01T00:00:00+00:00\",\"2025-01-02T00:00:00+00:00\",\"2025-01-03T00:00:00+00:00\",\"2025-01-04T00:00:00+00:00\",\"2025-01-05T00:00:00+00:00\",\"2025-01-06T00:00:00+00:00\",\"2025-01-07T00:00:00+00:00\",\"2025-01-08T00:00:00+00:00\",\"2025-01-09T00:00:00+00:00\",\"2025-01-10T00:00:00+00:00\",\"2025-01-11T00:00:00+00:00\",\"2025-01-12T00:00:00+00:00\",\"2025-01-13T00:00:00+00:00\",\"2025-01-14T00:00:00+00:00\",\"2025-01-15T00:00:00+00:00\",\"2025-01-16T00:00:00+00:00\",\"2025-01-17T00:00:00+00:00\",\"2025-01-18T00:00:00+00:00\",\"2025-01-19T00:00:00+00:00\",\"2025-01-20T00:00:00+00:00\",\"2025-01-21T00:00:00+00:00\",\"2025-01-22T00:00:00+00:00\",\"2025-01-23T00:00:00+00:00\",\"2025-01-24T00:00:00+00:00\",\"2025-01-25T00:00:00+00:00\",\"2025-01-26T00:00:00+00:00\",\"2025-01-27T00:00:00+00:00\",\"2025-01-28T00:00:00+00:00\",\"2025-01-29T00:00:00+00:00\",\"2025-01-30T00:00:00+00:00\",\"2025-01-31T00:00:00+00:00\",\"2025-02-01T00:00:00+00:00\",\"2025-02-02T00:00:00+00:00\",\"2025-02-03T00:00:00+00:00\",\"2025-02-04T00:00:00+00:00\",\"2025-02-05T00:00:00+00:00\",\"2025-02-06T00:00:00+00:00\",\"2025-02-07T00:00:00+00:00\",\"2025-02-08T00:00:00+00:00\",\"2025-02-09T00:00:00+00:00\",\"2025-02-10T00:00:00+00:00\",\"2025-02-11T00:00:00+00:00\",\"2025-02-12T00:00:00+00:00\",\"2025-02-13T00:00:00+00:00\",\"2025-02-14T00:00:00+00:00\",\"2025-02-15T00:00:00+00:00\",\"2025-02-16T00:00:00+00:00\",\"2025-02-17T00:00:00+00:00\",\"2025-02-18T00:00:00+00:00\",\"2025-02-19T00:00:00+00:00\",\"2025-02-20T00:00:00+00:00\",\"2025-02-21T00:00:00+00:00\",\"2025-02-22T00:00:00+00:00\",\"2025-02-23T00:00:00+00:00\",\"2025-02-24T00:00:00+00:00\",\"2025-02-25T00:00:00+00:00\",\"2025-02-26T00:00:00+00:00\",\"2025-02-27T00:00:00+00:00\",\"2025-02-28T00:00:00+00:00\",\"2025-03-01T00:00:00+00:00\",\"2025-03-02T00:00:00+00:00\",\"2025-03-03T00:00:00+00:00\",\"2025-03-04T00:00:00+00:00\",\"2025-03-05T00:00:00+00:00\",\"2025-03-06T00:00:00+00:00\",\"2025-03-07T00:00:00+00:00\",\"2025-03-08T00:00:00+00:00\",\"2025-03-09T00:00:00+00:00\",\"2025-03-10T00:00:00+00:00\",\"2025-03-11T00:00:00+00:00\",\"2025-03-12T00:00:00+00:00\",\"2025-03-13T00:00:00+00:00\",\"2025-03-14T00:00:00+00:00\",\"2025-03-15T00:00:00+00:00\",\"2025-03-16T00:00:00+00:00\",\"2025-03-17T00:00:00+00:00\",\"2025-03-18T00:00:00+00:00\",\"2025-03-19T00:00:00+00:00\",\"2025-03-20T00:00:00+00:00\",\"2025-03-21T00:00:00+00:00\",\"2025-03-22T00:00:00+00:00\",\"2025-03-23T00:00:00+00:00\",\"2025-03-24T00:00:00+00:00\",\"2025-03-25T00:00:00+00:00\",\"2025-03-26T00:00:00+00:00\",\"2025-03-27T00:00:00+00:00\",\"2025-03-28T00:00:00+00:00\",\"2025-03-29T00:00:00+00:00\",\"2025-03-30T00:00:00+00:00\",\"2025-03-31T00:00:00+00:00\",\"2025-04-01T00:00:00+00:00\",\"2025-04-02T00:00:00+00:00\",\"2025-04-03T00:00:00+00:00\",\"2025-04-04T00:00:00+00:00\",\"2025-04-05T00:00:00+00:00\",\"2025-04-06T00:00:00+00:00\",\"2025-04-07T00:00:00+00:00\",\"2025-04-08T00:00:00+00:00\",\"2025-04-09T00:00:00+00:00\",\"2025-04-10T00:00:00+00:00\",\"2025-04-11T00:00:00+00:00\",\"2025-04-12T00:00:00+00:00\",\"2025-04-13T00:00:00+00:00\",\"2025-04-14T00:00:00+00:00\",\"2025-04-15T00:00:00+00:00\",\"2025-04-16T00:00:00+00:00\",\"2025-04-17T00:00:00+00:00\",\"2025-04-18T00:00:00+00:00\",\"2025-04-19T00:00:00+00:00\",\"2025-04-20T00:00:00+00:00\",\"2025-04-21T00:00:00+00:00\",\"2025-04-22T00:00:00+00:00\",\"2025-04-23T00:00:00+00:00\",\"2025-04-24T00:00:00+00:00\",\"2025-04-25T00:00:00+00:00\",\"2025-04-26T00:00:00+00:00\",\"2025-04-27T00:00:00+00:00\",\"2025-04-28T00:00:00+00:00\",\"2025-04-29T00:00:00+00:00\",\"2025-04-30T00:00:00+00:00\",\"2025-05-01T00:00:00+00:00\",\"2025-05-02T00:00:00+00:00\",\"2025-05-03T00:00:00+00:00\",\"2025-05-04T00:00:00+00:00\",\"2025-05-05T00:00:00+00:00\",\"2025-05-06T00:00:00+00:00\",\"2025-05-07T00:00:00+00:00\",\"2025-05-08T00:00:00+00:00\",\"2025-05-09T00:00:00+00:00\",\"2025-05-10T00:00:00+00:00\",\"2025-05-11T00:00:00+00:00\",\"2025-05-12T00:00:00+00:00\",\"2025-05-13T00:00:00+00:00\",\"2025-05-14T00:00:00+00:00\",\"2025-05-15T00:00:00+00:00\",\"2025-05-16T00:00:00+00:00\",\"2025-05-17T00:00:00+00:00\",\"2025-05-18T00:00:00+00:00\",\"2025-05-19T00:00:00+00:00\",\"2025-05-20T00:00:00+00:00\",\"2025-05-21T00:00:00+00:00\",\"2025-05-22T00:00:00+00:00\",\"2025-05-23T00:00:00+00:00\",\"2025-05-24T00:00:00+00:00\",\"2025-05-25T00:00:00+00:00\",\"2025-05-26T00:00:00+00:00\",\"2025-05-27T00:00:00+00:00\",\"2025-05-28T00:00:00+00:00\",\"2025-05-29T00:00:00+00:00\",\"2025-05-30T00:00:00+00:00\",\"2025-05-31T00:00:00+00:00\",\"2025-06-01T00:00:00+00:00\",\"2025-06-02T00:00:00+00:00\",\"2025-06-03T00:00:00+00:00\",\"2025-06-04T00:00:00+00:00\",\"2025-06-05T00:00:00+00:00\",\"2025-06-06T00:00:00+00:00\",\"2025-06-07T00:00:00+00:00\",\"2025-06-08T00:00:00+00:00\",\"2025-06-09T00:00:00+00:00\",\"2025-06-10T00:00:00+00:00\",\"2025-06-11T00:00:00+00:00\",\"2025-06-12T00:00:00+00:00\",\"2025-06-13T00:00:00+00:00\",\"2025-06-14T00:00:00+00:00\",\"2025-06-15T00:00:00+00:00\",\"2025-06-16T00:00:00+00:00\",\"2025-06-17T00:00:00+00:00\",\"2025-06-18T00:00:00+00:00\",\"2025-06-19T00:00:00+00:00\",\"2025-06-20T00:00:00+00:00\",\"2025-06-21T00:00:00+00:00\",\"2025-06-22T00:00:00+00:00\",\"2025-06-23T00:00:00+00:00\",\"2025-06-24T00:00:00+00:00\",\"2025-06-25T00:00:00+00:00\",\"2025-06-26T00:00:00+00:00\",\"2025-06-27T00:00:00+00:00\",\"2025-06-28T00:00:00+00:00\",\"2025-06-29T00:00:00+00:00\",\"2025-06-30T00:00:00+00:00\",\"2025-07-01T00:00:00+00:00\",\"2025-07-02T00:00:00+00:00\",\"2025-07-03T00:00:00+00:00\",\"2025-07-04T00:00:00+00:00\",\"2025-07-05T00:00:00+00:00\",\"2025-07-06T00:00:00+00:00\",\"2025-07-07T00:00:00+00:00\",\"2025-07-08T00:00:00+00:00\",\"2025-07-09T00:00:00+00:00\",\"2025-07-10T00:00:00+00:00\",\"2025-07-11T00:00:00+00:00\",\"2025-07-12T00:00:00+00:00\",\"2025-07-13T00:00:00+00:00\",\"2025-07-14T00:00:00+00:00\",\"2025-07-15T00:00:00+00:00\",\"2025-07-16T00:00:00+00:00\",\"2025-07-17T00:00:00+00:00\",\"2025-07-18T00:00:00+00:00\",\"2025-07-19T00:00:00+00:00\",\"2025-07-20T00:00:00+00:00\",\"2025-07-21T00:00:00+00:00\",\"2025-07-22T00:00:00+00:00\",\"2025-07-23T00:00:00+00:00\",\"2025-07-24T00:00:30+00:00\",\"2025-07-25T00:00:03+00:00\",\"2025-07-26T00:00:21+00:00\",\"2025-07-27T00:00:22+00:00\",\"2025-07-28T00:00:27+00:00\",\"2025-07-29T00:00:10+00:00\",\"2025-07-30T00:00:45+00:00\",\"2025-07-31T00:00:55+00:00\",\"2025-08-01T00:00:07+00:00\",\"2025-08-02T00:00:53+00:00\",\"2025-08-03T00:00:12+00:00\",\"2025-08-04T00:00:10+00:00\",\"2025-08-05T00:00:20+00:00\",\"2025-08-06T00:00:06+00:00\",\"2025-08-07T00:00:19+00:00\",\"2025-08-08T00:00:20+00:00\",\"2025-08-09T00:00:49+00:00\",\"2025-08-10T00:00:20+00:00\",\"2025-08-11T00:00:07+00:00\",\"2025-08-12T00:00:05+00:00\",\"2025-08-13T00:00:30+00:00\",\"2025-08-14T00:00:29+00:00\",\"2025-08-15T00:00:29+00:00\",\"2025-08-16T00:00:06+00:00\",\"2025-08-17T00:00:08+00:00\",\"2025-08-18T00:00:13+00:00\",\"2025-08-19T00:00:40+00:00\",\"2025-08-20T00:00:07+00:00\",\"2025-08-21T00:00:15+00:00\",\"2025-08-22T00:00:50+00:00\",\"2025-08-23T00:00:55+00:00\",\"2025-08-24T00:00:57+00:00\",\"2025-08-25T00:00:01+00:00\",\"2025-08-26T00:00:45+00:00\",\"2025-08-27T00:00:56+00:00\",\"2025-08-28T00:00:31+00:00\",\"2025-08-29T00:00:55+00:00\",\"2025-08-30T00:00:12+00:00\",\"2025-08-31T00:00:14+00:00\",\"2025-09-01T00:00:17+00:00\",\"2025-09-02T00:00:56+00:00\",\"2025-09-03T00:00:33+00:00\",\"2025-09-04T00:00:23+00:00\",\"2025-09-05T00:00:14+00:00\",\"2025-09-06T00:00:13+00:00\",\"2025-09-07T00:00:21+00:00\",\"2025-09-08T00:00:24+00:00\",\"2025-09-09T00:00:25+00:00\",\"2025-09-10T00:00:05+00:00\",\"2025-09-11T00:00:55+00:00\",\"2025-09-12T00:00:21+00:00\",\"2025-09-13T00:00:19+00:00\",\"2025-09-14T00:00:58+00:00\",\"2025-09-15T00:00:01+00:00\",\"2025-09-16T00:00:06+00:00\",\"2025-09-17T00:00:00+00:00\",\"2025-09-18T00:00:34+00:00\",\"2025-09-19T00:00:47+00:00\",\"2025-09-20T00:00:03+00:00\",\"2025-09-21T00:00:06+00:00\",\"2025-09-22T00:00:17+00:00\",\"2025-09-23T00:00:13+00:00\",\"2025-09-24T00:00:13+00:00\",\"2025-09-25T00:00:26+00:00\",\"2025-09-26T00:00:13+00:00\",\"2025-09-27T00:00:14+00:00\",\"2025-09-28T00:00:18+00:00\",\"2025-09-29T00:00:22+00:00\",\"2025-09-30T00:00:56+00:00\",\"2025-10-01T00:00:07+00:00\",\"2025-10-02T00:00:54+00:00\",\"2025-10-03T01:00:58+00:00\",\"2025-10-04T00:00:36+00:00\",\"2025-10-05T00:00:13+00:00\",\"2025-10-06T00:00:17+00:00\",\"2025-10-07T00:00:26+00:00\",\"2025-10-08T00:00:31+00:00\",\"2025-10-09T00:00:45+00:00\",\"2025-10-10T00:00:05+00:00\",\"2025-10-11T00:00:30+00:00\",\"2025-10-12T00:00:34+00:00\",\"2025-10-13T00:00:37+00:00\",\"2025-10-14T00:00:16+00:00\",\"2025-10-15T00:00:56+00:00\",\"2025-10-16T00:00:55+00:00\",\"2025-10-17T00:00:57+00:00\",\"2025-10-18T00:00:13+00:00\",\"2025-10-19T00:00:48+00:00\",\"2025-10-20T00:00:01+00:00\",\"2025-10-21T00:00:37+00:00\",\"2025-10-22T00:00:09+00:00\",\"2025-10-23T00:00:11+00:00\",\"2025-10-24T00:00:10+00:00\",\"2025-10-25T00:00:16+00:00\",\"2025-10-26T00:00:53+00:00\",\"2025-10-27T00:00:36+00:00\",\"2025-10-28T00:00:15+00:00\",\"2025-10-29T00:00:15+00:00\",\"2025-10-30T00:00:22+00:00\",\"2025-10-31T00:00:30+00:00\",\"2025-11-01T00:00:13+00:00\",\"2025-11-02T00:00:16+00:00\",\"2025-11-03T00:00:18+00:00\",\"2025-11-04T00:00:21+00:00\",\"2025-11-05T00:00:58+00:00\",\"2025-11-06T00:00:01+00:00\",\"2025-11-07T00:00:36+00:00\",\"2025-11-08T00:00:52+00:00\",\"2025-11-09T00:00:54+00:00\",\"2025-11-10T00:00:56+00:00\",\"2025-11-11T00:00:01+00:00\",\"2025-11-12T00:00:23+00:00\",\"2025-11-13T00:00:24+00:00\",\"2025-11-14T00:00:26+00:00\",\"2025-11-15T00:00:26+00:00\",\"2025-11-16T00:00:19+00:00\",\"2025-11-17T00:00:10+00:00\",\"2025-11-18T00:00:08+00:00\",\"2025-11-19T00:00:11+00:00\",\"2025-11-20T00:00:39+00:00\",\"2025-11-21T00:00:33+00:00\",\"2025-11-22T00:00:09+00:00\",\"2025-11-23T00:00:03+00:00\",\"2025-11-24T00:00:06+00:00\",\"2025-11-25T00:00:10+00:00\",\"2025-11-26T00:00:14+00:00\",\"2025-11-27T00:00:33+00:00\",\"2025-11-28T00:00:15+00:00\",\"2025-11-29T00:00:19+00:00\",\"2025-11-30T00:00:23+00:00\",\"2025-12-01T00:00:44+00:00\",\"2025-12-02T00:00:49+00:00\",\"2025-12-03T00:00:51+00:00\",\"2025-12-04T00:00:24+00:00\",\"2025-12-05T00:00:25+00:00\",\"2025-12-06T00:00:42+00:00\",\"2025-12-07T00:00:09+00:00\",\"2025-12-08T00:00:41+00:00\",\"2025-12-09T00:00:28+00:00\",\"2025-12-10T00:00:44+00:00\",\"2025-12-11T00:00:57+00:00\",\"2025-12-12T00:00:40+00:00\",\"2025-12-13T00:00:41+00:00\"],\"xaxis\":\"x\",\"y\":[100990.4634993516,101407.92194099003,101421.90351821724,104399.97241572764,105976.87174120503,106136.79384351085,100188.70906076657,97680.67927431843,97709.62346981636,97297.31920022352,95180.17757518878,94746.99672280472,98672.40333026911,99313.16554239673,95737.02133096024,94258.28225803278,95208.16893002423,93638.1183296515,92669.30759511188,93400.35434127443,94440.90256707114,96894.17887128204,98114.29986321744,98236.00626138304,98300.841308766,102246.65531487555,96965.56108405908,95101.01517436972,92532.884999129,94764.79486842193,94588.4542499602,94482.49176129332,94482.49176129332,94482.49176129332,94482.49176129332,94482.49176129332,94482.49176129332,94482.49176129332,94482.49176129332,102115.18349457298,106177.01139051635,103695.91235094184,103969.71820454052,104885.85452796306,104791.68065154876,102703.1701266888,102059.36263997732,101306.37388634228,103759.78212090959,104789.15943756569,102399.08435498596,100675.40952090696,97680.98880411168,101429.89156225936,97885.80639019617,96615.3122452453,96593.24446032189,96517.9570752472,96515.70109762916,96492.1960670252,97448.15265444468,95781.05671247291,97847.23682283088,96650.83441878276,97505.66700988411,97603.34950011734,96196.35117110558,95798.80322433353,95522.01863498973,96635.56086660107,98344.81588337972,96130.70971461621,96593.37496290675,96274.25595347586,91594.02139234803,88673.6778061724,84049.11667926899,84628.75184377439,84339.43620653912,86001.13885335272,94256.52034184404,86174.99450255393,87256.19692972445,90635.64842275524,89981.91015145795,86740.29481743161,86183.23955048165,80653.01746495762,78629.40230328,82835.08764886283,83707.48573153565,81083.5868325083,84006.36074423071,84373.93990732651,82592.51641422704,84046.85809846506,82726.59324859978,86859.96436505626,84208.82772550809,84073.63186822298,83840.39527371668,85939.3131663287,87469.69863323083,87459.89649789878,86905.86814754657,87202.98577094653,84357.07107135889,82609.772037966,82342.51119937954,82570.45601432286,85272.86909770302,82528.30574186488,83099.31329558417,83829.85024986227,83479.9790747844,78144.52335526087,79283.40590069917,76306.64297400902,82629.27954597674,79602.64906113624,83407.19002506316,85282.79121693141,83635.8906303313,84533.2271297906,83674.42604335882,84072.38143863062,84916.27893725,84455.17307349702,85108.00223982787,85107.44008581626,87477.71006496607,93518.30781947899,93660.90847737007,93820.08035448579,94752.48858437868,94682.13753488561,93805.03363922455,95017.41860482188,94207.78507067182,94213.94854781654,96457.30779989906,96879.79596914822,95941.9257721482,94402.35210862252,94777.6526269083,96804.93234341647,97060.56470693901,103167.60093600699,102960.46895176245,104527.87456579873,104023.77008207429,102861.92435732382,104168.56299848325,103563.30124518473,103788.91045893375,103528.54674374008,103191.84401476309,106481.35907098731,105626.00966006031,106822.72143647949,109692.57249506621,111701.20615302924,107308.19461539254,107846.51489423368,109037.00643162607,109502.57896586903,109032.48530710112,107859.27969287951,105631.23238846016,104060.21527630632,104685.35791244401,105707.96302934791,105924.3249829807,105487.57253965682,104801.93090868031,101620.18829904252,104406.7000496207,105650.76452461252,105788.37267227533,110307.89270712654,110287.13000427833,108725.86508355924,105864.55972967156,106142.93750139237,105486.96921978821,105613.04472659169,106910.68698519835,104631.04596931994,104926.09194630527,104727.02554329904,103338.1079197114,102098.71989132554,101008.83387117805,105481.64588095516,106101.78305197463,107386.37810486283,107006.00526201789,107116.50411636064,107359.34437466756,108387.31200670116,107213.29685581694,105732.88526612076,108895.24427569262,109663.43119273224,108063.83049796481,108273.34676256684,109262.61463810524,108327.8427585979,108981.81972644481,111314.78523603403,116011.38055155263,117587.29461890951,117450.39932475804,119130.39475255577,119910.4512875028,117741.30248451102,118759.28042999613,119321.11862920444,118030.14320476272,117925.93613842786,117348.76501653827,117479.1213243608,120021.82932823864,118756.0,118340.1,117629.9,117944.3,119443.77,117947.97,117946.4,117840.29,115772.0,113329.63,112558.4,114214.4,115072.43,114143.77,115028.3,117499.8,116695.66,116500.0,119288.0,118715.99,120177.0,123388.25,118348.66,117349.0,117482.6,117454.86,116241.94,112820.75,114268.68,112449.3,116911.28,115406.0,113464.01,110138.5,111779.81,111223.61,112565.33,108415.7,108815.0,108244.01,109263.55,111215.6,111704.66,110709.0,110658.0,110222.2,111172.0,112082.93,111515.2,113979.1,115484.31,116099.97,115968.46,115370.0,115431.35,116796.95,116447.65,117108.03,115632.38,115705.9,115300.0,112698.8,112012.01,113299.1,109011.51,109699.98,109648.22,112125.0,114380.8,114048.94,118576.81511910717,120153.40479895372,122233.54,122382.163,123488.56,124738.52,121421.85,123339.0,121645.64,113103.8,110785.3,115085.04,115191.7,113072.69,110745.62,108159.73862428806,106449.59,107191.3,108636.8,110575.9,108438.88,107671.0,110032.5,111008.25663007,111634.60500000001,114509.45,114133.84,112917.0,110040.01,108315.87,109609.22,110112.4,110646.32,106587.91,101532.0,103930.88,101319.22,103347.54,102312.94,104730.09,106014.33,103039.61,101655.93,99754.43,94478.0,95577.6518608,94255.2,92141.0,92990.0,91502.85,86587.51,85123.98,84691.43,86825.1,88290.0,87371.216248,90518.01,91303.10902574495,90917.84,90831.72,90380.0,86328.0,91328.01,93484.12,92131.04475,89368.32,89255.78,90413.0,90639.995,92696.54,91990.74,92498.67,90267.78],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Date\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Price (USD)\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Bitcoin Price Over 2025\"},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b577368f-1b29-4fcb-98c8-24567bcbe0ba');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}